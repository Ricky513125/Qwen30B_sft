"""
è®­ç»ƒå™¨æ¨¡å— - ä¼˜åŒ–ç‰ˆ
é€‚é…æ¶ˆèå®éªŒï¼Œæ”¯æŒä¸¥æ ¼çš„è§’è‰²æ§åˆ¶ä¸æ—¥å¿—ç›‘æ§
"""
import os
import re
import time
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
)
from typing import List, Dict, Any, Optional, Tuple
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ prompt_builder
sys.path.insert(0, str(Path(__file__).parent))
from prompt_builder import build_training_prompt


class AblationDataset(Dataset):
    def __init__(self, samples, tokenizer, max_length=32768, use_profile=True, use_history=True, use_context=True):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. åˆå§‹æ„å»º
        messages, target_answer = build_training_prompt(
            context=sample['context'],
            next_question=sample['next_question'],
            user_profile=sample.get('user_profile') if self.use_profile else None,
            task_description=sample.get('task_description'),
            history=sample.get('history') if self.use_history else None,
            use_profile=self.use_profile,
            use_history=self.use_history,
            use_context=self.use_context
        )

        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šåŠ¨æ€è£å‰ªå†å²ä»¥é˜²æ­¢æˆªæ–­ ---
        # å¦‚æœæ¶ˆæ¯å¤ªé•¿ï¼Œå¾ªç¯åˆ é™¤ messages ä¸­æœ€æ—©çš„å¯¹è¯è½®æ¬¡ï¼ˆä¿ç•™ system æç¤ºè¯ï¼‰
        # ç´¢å¼• 0 æ˜¯ systemï¼Œä¹‹åæ˜¯ user/assistant äº¤æ›¿
        # ä¸ºäº†ä¿æŒè§’è‰²äº¤æ›¿ï¼Œéœ€è¦æˆå¯¹åˆ é™¤ï¼ˆuser+assistantï¼‰
        max_iterations = 100  # é˜²æ­¢æ— é™å¾ªç¯
        iteration = 0
        while iteration < max_iterations:
            try:
                tokenized_length = len(self.tokenizer.apply_chat_template(messages, tokenize=True))
                if tokenized_length <= (self.max_length - 512):
                    break
            except Exception as e:
                # å¦‚æœ apply_chat_template å¤±è´¥ï¼Œè¯´æ˜æ¶ˆæ¯æ ¼å¼æœ‰é—®é¢˜
                # å°è¯•é‡æ–°è§„èŒƒåŒ–æ¶ˆæ¯
                from prompt_builder import _normalize_messages
                messages = _normalize_messages(messages)
                # å¦‚æœè§„èŒƒåŒ–åä»ç„¶å¤±è´¥ï¼Œè·³å‡ºå¾ªç¯
                try:
                    tokenized_length = len(self.tokenizer.apply_chat_template(messages, tokenize=True))
                    if tokenized_length <= (self.max_length - 512):
                        break
                except:
                    print(f"è­¦å‘Š: æ— æ³•è§„èŒƒåŒ–æ¶ˆæ¯æ ¼å¼ï¼Œè·³è¿‡è£å‰ª: {e}")
                    break
            
            if len(messages) > 3:  # system + è‡³å°‘ä¸€å¯¹ user/assistant
                # æ£€æŸ¥ç´¢å¼• 1 æ˜¯å¦æ˜¯ userï¼Œç´¢å¼• 2 æ˜¯å¦æ˜¯ assistant
                if messages[1].get('role') == 'user' and len(messages) > 2 and messages[2].get('role') == 'assistant':
                    # åˆ é™¤ä¸€å¯¹ user/assistantï¼ˆä¿æŒäº¤æ›¿ï¼‰
                    messages.pop(1)  # åˆ é™¤ user
                    messages.pop(1)  # åˆ é™¤ assistantï¼ˆç°åœ¨ç´¢å¼• 1 çš„ä½ç½®ï¼‰
                else:
                    # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œåªåˆ é™¤ä¸€æ¡ï¼ˆå‘åå…¼å®¹ï¼Œä½†å¯èƒ½ç ´åäº¤æ›¿ï¼‰
                    # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸º normalize_messages å·²ç»ç¡®ä¿äº†äº¤æ›¿
                    messages.pop(1)
            else:
                break
            iteration += 1

        # 2. ç”Ÿæˆ Prompt
        # ç¡®ä¿ messages ä»¥ assistant ç»“å°¾ï¼ˆç”¨äºé¢„æµ‹ç”¨æˆ·ä¸‹ä¸€å¥è¯ï¼‰
        # å¦‚æœæœ€åä¸€æ¡ä¸æ˜¯ assistantï¼Œè¯´æ˜æœ‰é—®é¢˜ï¼Œåº”è¯¥å·²ç»è¢« normalize_messages å¤„ç†äº†
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        
        # 3. æ‰‹åŠ¨æ·»åŠ  "user: " æç¤ºï¼Œè®©æ¨¡å‹é¢„æµ‹ç”¨æˆ·ä¼šè¯´ä»€ä¹ˆ
        # æ³¨æ„ï¼šGemma çš„ chat template ä½¿ç”¨ <start_of_turn> å’Œ <end_of_turn>ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ç®€å•çš„ "user: " æ ¼å¼
        generation_suffix = "\nuser: "

        # 4. ç»„åˆæˆçœŸæ­£çš„ Prompt
        full_prompt = full_prompt.strip() + generation_suffix
        # ç¡®ä¿ä¸åŒ…å«ç­”æ¡ˆï¼Œä½¿ç”¨ <|im_end|> ä½œä¸ºç»“æŸæ ‡è®°ï¼ˆè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢ï¼‰
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token

        # 3. ç¼–ç 
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()

        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šé«˜ç²¾åº¦è®¡ç®— Prompt é•¿åº¦ ---
        # æˆ‘ä»¬ä¸ç›´æ¥ encode(full_prompt)ï¼Œè€Œæ˜¯é€šè¿‡å¯»æ‰¾ target çš„èµ·å§‹ token æ¥ç¡®å®š
        target_ids = self.tokenizer.encode(target_answer, add_special_tokens=False)
        
        # å¯»æ‰¾åˆ†ç•Œç‚¹ï¼šåœ¨ input_ids ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸å±äº prompt çš„ä½ç½®
        # æˆ‘ä»¬å¯ä»¥å…ˆ encode ä¸€ä¸ªå®Œå…¨æ²¡å¸¦ç‰¹æ®Šå­—ç¬¦çš„ prompt
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)

        labels = input_ids.clone()
        
        # å±è”½ Promptï¼šç¡®ä¿ä¸ä¼šè¶Šç•Œ
        safe_prompt_len = min(actual_prompt_len, self.max_length - 1)
        labels[:safe_prompt_len] = -100
        
        # å±è”½ Padding
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        # --- å±è”½ç‰¹æ®Š Token (ä¿ç•™ EOS å’Œ <|im_end|>) ---
        # è·å– <|im_end|> çš„ token IDï¼Œç¡®ä¿å®ƒè¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­
        im_end_token = "<|im_end|>"
        im_end_id = None
        try:
            # å°è¯•è·å– <|im_end|> çš„ token ID
            im_end_ids = self.tokenizer.encode(im_end_token, add_special_tokens=False)
            if im_end_ids:
                im_end_id = im_end_ids[0]  # é€šå¸¸ <|im_end|> æ˜¯ä¸€ä¸ªå•ç‹¬çš„ token
                # è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°ï¼‰
                if not hasattr(self, '_im_end_logged'):
                    print(f"âœ“ <|im_end|> token ID: {im_end_id}ï¼Œå°†è¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                    self._im_end_logged = True
        except Exception as e:
            if not hasattr(self, '_im_end_error_logged'):
                print(f"è­¦å‘Š: æ— æ³•è·å– <|im_end|> token ID: {e}")
                self._im_end_error_logged = True
        
        special_ids = set(self.tokenizer.all_special_ids)
        eos_id = self.tokenizer.eos_token_id
        # ä¿ç•™ EOS å’Œ <|im_end|> tokenï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢
        tokens_to_keep = {eos_id}
        if im_end_id is not None:
            tokens_to_keep.add(im_end_id)
        
        for tid in special_ids:
            if tid not in tokens_to_keep:
                labels[labels == tid] = -100
        
        # éªŒè¯ <|im_end|> æ˜¯å¦åœ¨ labels ä¸­ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if im_end_id is not None and (labels == im_end_id).any():
            if not hasattr(self, '_im_end_verified'):
                print(f"âœ“ ç¡®è®¤: <|im_end|> token (ID: {im_end_id}) å·²åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                self._im_end_verified = True

        # 4. æœ€ç»ˆéªŒè¯ï¼šé˜²æ­¢ NaN
        if (labels != -100).sum() == 0:
            # æŒ½æ•‘é€»è¾‘ï¼šå¦‚æœå…¨è¢«å±è”½äº†ï¼ˆè¯´æ˜æˆªæ–­å¤ªä¸¥é‡ï¼‰ï¼Œå¼ºè¡Œæš´éœ²æœ€å 32 ä¸ª token 
            # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ç­”æ¡ˆæé•¿æˆ–æˆªæ–­åˆšå¥½åˆ‡åœ¨äº†ç­”æ¡ˆå¼€å¤´
            labels[-32:] = input_ids[-32:]
            labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }


class CustomTrainer(Trainer):
    """å¸¦å®æ—¶æ—¥å¿—çš„è‡ªå®šä¹‰è®­ç»ƒå™¨"""
    
    def __init__(self, *args, verbose_logging=False, log_file_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.verbose_logging = verbose_logging
        self.log_file_path = log_file_path
        self.log_entry_count = 0
        
        if self.log_file_path:
            os.makedirs(os.path.dirname(self.log_file_path), exist_ok=True)
            self.log_file = open(self.log_file_path, 'w', encoding='utf-8')
            self.log_file.write("[\n")

    def __del__(self):
        if hasattr(self, 'log_file') and self.log_file:
            try:
                self.log_file.write("\n]")
                self.log_file.close()
            except: pass

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        outputs = model(**inputs)
        loss = outputs.loss if hasattr(outputs, 'loss') else None
        
        if loss is None and "labels" in inputs:
            logits = outputs.get("logits")
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["labels"][..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        if self.verbose_logging and (self.state.global_step % self.args.logging_steps == 0):
            self._log_details(inputs, outputs, loss.item())

        return (loss, outputs) if return_outputs else loss

    def clean_output_text(self, text: str) -> str:
        # ç§»é™¤æ€è€ƒè¿‡ç¨‹
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = text.replace('<think>', '').replace('</think>', '')
        return text.strip()

    def _log_details(self, inputs, outputs, loss_val):
        """è®°å½•è®­ç»ƒç»†èŠ‚ï¼šå¯¹æ¯” Target å’Œæ¨¡å‹çš„é¢„æµ‹ (Argmax)"""
        try:
            batch_idx = 0
            ids = inputs['input_ids'][batch_idx]
            lbs = inputs['labels'][batch_idx]
            logits = outputs.get("logits")[batch_idx]
            
            # è§£ç  Target
            target_ids = [t.item() for t in lbs if t != -100]
            target_text = self.tokenizer.decode(target_ids, skip_special_tokens=True)
            
            # è§£ç é¢„æµ‹ (å¯»æ‰¾ label æœ‰æ•ˆä½å¯¹åº”çš„é¢„æµ‹ä½)
            pred_ids_all = logits.argmax(dim=-1)
            valid_pos = (lbs != -100).nonzero(as_tuple=True)[0]
            pred_ids = [pred_ids_all[p-1].item() for p in valid_pos if p > 0]
            predict_text = self.tokenizer.decode(pred_ids, skip_special_tokens=True)
            
            print(f"\n[Step {self.state.global_step}] Loss: {loss_val:.4f}")
            print(f"ğŸ¯ Target: {target_text[:100]}")
            print(f"ğŸ¤– Predict: {predict_text[:100]}")

            if hasattr(self, 'log_file'):
                log_data = {
                    "step": self.state.global_step,
                    "loss": loss_val,
                    "target": target_text,
                    "predict": predict_text
                }
                if self.log_entry_count > 0: self.log_file.write(",\n")
                self.log_file.write(json.dumps(log_data, ensure_ascii=False))
                self.log_file.flush()
                self.log_entry_count += 1
        except Exception as e:
            print(f"Log Error: {e}")


class AblationTrainer:
    """æ¶ˆèå®éªŒä¸»æ§ç±»"""
    
    def __init__(self, model_path: str, output_dir: str, config: Dict[str, Any], 
                 use_profile: bool = True, use_history: bool = True, use_context: bool = True, 
                 log_file_path: Optional[str] = None, use_quantization: bool = True):
        self.model_path = model_path
        self.output_dir = output_dir
        self.config = config
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        self.log_file_path = log_file_path
        self.use_quantization = use_quantization

        # 1. åŠ è½½ Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 2. é…ç½®é‡åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        quantization_config = None
        if self.use_quantization:
            try:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,  # H100 å¿…é€‰ bf16
                    bnb_4bit_use_double_quant=True,
                )
                print("âœ“ å¯ç”¨ 4-bit é‡åŒ– (NF4)")
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•å¯ç”¨é‡åŒ–ï¼Œå°†ä½¿ç”¨å…¨ç²¾åº¦: {e}")
                quantization_config = None
                self.use_quantization = False
        
        # 3. åŠ è½½æ¨¡å‹
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "device_map": "auto" if self.use_quantization else None,
        }
        
        # æ·»åŠ é‡åŒ–é…ç½®
        if quantization_config is not None:
            model_kwargs["quantization_config"] = quantization_config
        else:
            # å¦‚æœæ²¡æœ‰é‡åŒ–ï¼Œä½¿ç”¨ bfloat16 æˆ– float16
            model_kwargs["torch_dtype"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        
        # å°è¯•ä½¿ç”¨ Flash Attention 2
        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† flash-attn
        try:
            import flash_attn
            model_kwargs["attn_implementation"] = "flash_attention_2"
            print("âœ“ å¯ç”¨ Flash Attention 2")
        except ImportError:
            print("âš  Flash Attention 2 æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›å®ç°")
            print("  æç¤º: å®‰è£… flash-attn å¯ä»¥æå‡è®­ç»ƒé€Ÿåº¦å’Œé™ä½æ˜¾å­˜å ç”¨")
        except Exception as e:
            print(f"âš  æ— æ³•å¯ç”¨ Flash Attention 2: {e}")
            print("  å°†ä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›å®ç°")
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        
        # å¦‚æœæ²¡æœ‰ä½¿ç”¨ device_mapï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if not self.use_quantization:
            self.model = self.model.to(self.device)
        
        # 4. æ˜¾å­˜ä¼˜åŒ–è®¾ç½®
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("âœ“ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")

    def train(self, train_samples: List[Dict[str, Any]], val_samples: Optional[List[Dict[str, Any]]] = None):
        train_config = self.config.get('training', {})
        
        train_dataset = AblationDataset(
            train_samples, self.tokenizer, 
            max_length=train_config.get('max_length', 32768),
            use_profile=self.use_profile, use_history=self.use_history, use_context=self.use_context
        )

        # ä¼˜åŒ–è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=train_config.get('num_epochs', 3),
            per_device_train_batch_size=train_config.get('batch_size', 1),
            gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 4),
            learning_rate=train_config.get('learning_rate', 2e-4),
            logging_steps=train_config.get('logging_steps', 10),
            save_steps=train_config.get('save_steps', 500),
            save_total_limit=train_config.get('save_total_limit', 3),
            # H100 å¿…é¡»ç”¨ bf16 æ‰èƒ½å‘æŒ¥æ€§èƒ½
            bf16=torch.cuda.is_bf16_supported(),
            fp16=False,  # ä¸ä½¿ç”¨ fp16ï¼Œä¼˜å…ˆä½¿ç”¨ bf16
            # èŠ‚çœä¼˜åŒ–å™¨æ˜¾å­˜
            optim="paged_adamw_8bit" if self.use_quantization else "adamw_torch",
            # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆåœ¨æ¨¡å‹åŠ è½½æ—¶å·²å¯ç”¨ï¼Œè¿™é‡Œä¹Ÿè®¾ç½®ï¼‰
            gradient_checkpointing=True,
            # æœ€å¤§åºåˆ—é•¿åº¦
            max_steps=train_config.get('max_steps', -1),
            warmup_steps=train_config.get('warmup_steps', 100),
            weight_decay=train_config.get('weight_decay', 0.1),
            report_to="none",
            remove_unused_columns=False,
            # å…¶ä»–ä¼˜åŒ–è®¾ç½®
            dataloader_pin_memory=True,
            dataloader_num_workers=4,
        )

        trainer = CustomTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            verbose_logging=True,
            log_file_path=self.log_file_path
        )

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: Profile={self.use_profile}, History={self.use_history}, Context={self.use_context}")
        trainer.train()
        
        # ä¿å­˜
        trainer.save_model(self.output_dir)
        self.tokenizer.save_pretrained(self.output_dir)