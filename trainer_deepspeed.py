"""
è®­ç»ƒå™¨æ¨¡å— - DeepSpeed ZeRO-3 å¤š GPU ç‰ˆæœ¬
æ”¯æŒè‡ªåŠ¨æ£€æµ‹ç©ºé—² GPUï¼Œæœ€å¤šä½¿ç”¨ 4 å¼  GPU
"""
import os
import re
import time
import json
import torch
import torch.nn as nn
import subprocess
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    # HfDeepSpeedConfig,
)
# ä¸å†éœ€è¦æ‰‹åŠ¨å¯¼å…¥ HfDeepSpeedConfigï¼ŒTrainer ä¼šè‡ªåŠ¨å¤„ç†
from typing import List, Dict, Any, Optional, Tuple
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ prompt_builder
sys.path.insert(0, str(Path(__file__).parent))
from prompt_builder import build_training_prompt


def get_gpu_memory_usage():
    """è·å–æ‰€æœ‰ GPU çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', 
             '--format=csv,noheader,nounits'],
            capture_output=True,
            text=True,
            check=True
        )
        
        gpu_info = []
        for line in result.stdout.strip().split('\n'):
            if line.strip():
                parts = line.split(', ')
                if len(parts) >= 4:
                    gpu_id = int(parts[0])
                    memory_used = int(parts[1])
                    memory_total = int(parts[2])
                    utilization = int(parts[3])
                    memory_free = memory_total - memory_used
                    memory_usage_percent = (memory_used / memory_total) * 100
                    
                    gpu_info.append({
                        'id': gpu_id,
                        'memory_used': memory_used,
                        'memory_total': memory_total,
                        'memory_free': memory_free,
                        'memory_usage_percent': memory_usage_percent,
                        'utilization': utilization
                    })
        return gpu_info
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è·å– GPU ä¿¡æ¯: {e}")
        return []


def find_free_gpus(max_gpus=4, memory_threshold_mb=1000, utilization_threshold=10):
    """
    è‡ªåŠ¨æ£€æµ‹ç©ºé—²çš„ GPU
    
    Args:
        max_gpus: æœ€å¤šä½¿ç”¨çš„ GPU æ•°é‡
        memory_threshold_mb: æ˜¾å­˜ä½¿ç”¨é˜ˆå€¼ï¼ˆMBï¼‰ï¼Œä½äºæ­¤å€¼è®¤ä¸º GPU ç©ºé—²
        utilization_threshold: GPU åˆ©ç”¨ç‡é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œä½äºæ­¤å€¼è®¤ä¸º GPU ç©ºé—²
    
    Returns:
        ç©ºé—² GPU ID åˆ—è¡¨
    """
    gpu_info = get_gpu_memory_usage()
    
    if not gpu_info:
        print("è­¦å‘Š: æ— æ³•æ£€æµ‹ GPUï¼Œå°†ä½¿ç”¨é»˜è®¤ GPU")
        return [0] if torch.cuda.is_available() else []
    
    # ç­›é€‰ç©ºé—² GPU
    free_gpus = []
    for gpu in gpu_info:
        if (gpu['memory_free'] > memory_threshold_mb and 
            gpu['utilization'] < utilization_threshold):
            free_gpus.append(gpu)
    
    # æŒ‰æ˜¾å­˜ç©ºé—²é‡æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
    free_gpus.sort(key=lambda x: x['memory_free'], reverse=True)
    
    # é€‰æ‹©æœ€å¤š max_gpus ä¸ª GPU
    selected_gpus = free_gpus[:max_gpus]
    gpu_ids = [gpu['id'] for gpu in selected_gpus]
    
    if gpu_ids:
        print(f"âœ“ æ£€æµ‹åˆ° {len(gpu_ids)} ä¸ªç©ºé—² GPU: {gpu_ids}")
        for gpu in selected_gpus:
            print(f"  GPU {gpu['id']}: æ˜¾å­˜ {gpu['memory_free']}MB ç©ºé—², åˆ©ç”¨ç‡ {gpu['utilization']}%")
    else:
        print("âš  æœªæ£€æµ‹åˆ°ç©ºé—² GPUï¼Œå°†ä½¿ç”¨ GPU 0")
        gpu_ids = [0] if torch.cuda.is_available() else []
    
    return gpu_ids


class AblationDataset(Dataset):
    def __init__(self, samples, tokenizer, max_length=32768, use_profile=True, use_history=True, use_context=True):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. åˆå§‹æ„å»º
        messages, target_answer = build_training_prompt(
            context=sample['context'],
            next_question=sample['next_question'],
            user_profile=sample.get('user_profile') if self.use_profile else None,
            task_description=sample.get('task_description'),
            history=sample.get('history') if self.use_history else None,
            use_profile=self.use_profile,
            use_history=self.use_history,
            use_context=self.use_context
        )

        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šåŠ¨æ€è£å‰ªå†å²ä»¥é˜²æ­¢æˆªæ–­ ---
        # å¦‚æœæ¶ˆæ¯å¤ªé•¿ï¼Œå¾ªç¯åˆ é™¤ messages ä¸­æœ€æ—©çš„å¯¹è¯è½®æ¬¡ï¼ˆä¿ç•™ system æç¤ºè¯ï¼‰
        # ç´¢å¼• 0 æ˜¯ systemï¼Œä¹‹åæ˜¯ user/assistant äº¤æ›¿
        # ä¸ºäº†ä¿æŒè§’è‰²äº¤æ›¿ï¼Œéœ€è¦æˆå¯¹åˆ é™¤ï¼ˆuser+assistantï¼‰
        max_iterations = 100  # é˜²æ­¢æ— é™å¾ªç¯
        iteration = 0
        while iteration < max_iterations:
            try:
                tokenized_length = len(self.tokenizer.apply_chat_template(messages, tokenize=True))
                if tokenized_length <= (self.max_length - 512):
                    break
            except Exception as e:
                # å¦‚æœ apply_chat_template å¤±è´¥ï¼Œè¯´æ˜æ¶ˆæ¯æ ¼å¼æœ‰é—®é¢˜
                # å°è¯•é‡æ–°è§„èŒƒåŒ–æ¶ˆæ¯
                from prompt_builder import _normalize_messages
                messages = _normalize_messages(messages)
                # å¦‚æœè§„èŒƒåŒ–åä»ç„¶å¤±è´¥ï¼Œè·³å‡ºå¾ªç¯
                try:
                    tokenized_length = len(self.tokenizer.apply_chat_template(messages, tokenize=True))
                    if tokenized_length <= (self.max_length - 512):
                        break
                except:
                    print(f"è­¦å‘Š: æ— æ³•è§„èŒƒåŒ–æ¶ˆæ¯æ ¼å¼ï¼Œè·³è¿‡è£å‰ª: {e}")
                    break
            
            if len(messages) > 3:  # system + è‡³å°‘ä¸€å¯¹ user/assistant
                # æ£€æŸ¥ç´¢å¼• 1 æ˜¯å¦æ˜¯ userï¼Œç´¢å¼• 2 æ˜¯å¦æ˜¯ assistant
                if messages[1].get('role') == 'user' and len(messages) > 2 and messages[2].get('role') == 'assistant':
                    # åˆ é™¤ä¸€å¯¹ user/assistantï¼ˆä¿æŒäº¤æ›¿ï¼‰
                    messages.pop(1)  # åˆ é™¤ user
                    messages.pop(1)  # åˆ é™¤ assistantï¼ˆç°åœ¨ç´¢å¼• 1 çš„ä½ç½®ï¼‰
                else:
                    # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œåªåˆ é™¤ä¸€æ¡ï¼ˆå‘åå…¼å®¹ï¼Œä½†å¯èƒ½ç ´åäº¤æ›¿ï¼‰
                    # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸º normalize_messages å·²ç»ç¡®ä¿äº†äº¤æ›¿
                    messages.pop(1)
            else:
                break
            iteration += 1

        # 2. ç”Ÿæˆ Prompt
        # ç¡®ä¿ messages ä»¥ assistant ç»“å°¾ï¼ˆç”¨äºé¢„æµ‹ç”¨æˆ·ä¸‹ä¸€å¥è¯ï¼‰
        # å¦‚æœæœ€åä¸€æ¡ä¸æ˜¯ assistantï¼Œè¯´æ˜æœ‰é—®é¢˜ï¼Œåº”è¯¥å·²ç»è¢« normalize_messages å¤„ç†äº†
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        
        # 3. æ‰‹åŠ¨æ·»åŠ  "user: " æç¤ºï¼Œè®©æ¨¡å‹é¢„æµ‹ç”¨æˆ·ä¼šè¯´ä»€ä¹ˆ
        # æ³¨æ„ï¼šGemma çš„ chat template ä½¿ç”¨ <start_of_turn> å’Œ <end_of_turn>ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ç®€å•çš„ "user: " æ ¼å¼
        generation_suffix = "\nuser: "

        # 4. ç»„åˆæˆçœŸæ­£çš„ Prompt
        full_prompt = full_prompt.strip() + generation_suffix
        # ç¡®ä¿ä¸åŒ…å«ç­”æ¡ˆï¼Œä½¿ç”¨ <|im_end|> ä½œä¸ºç»“æŸæ ‡è®°ï¼ˆè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢ï¼‰
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token

        # 3. ç¼–ç 
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()

        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šé«˜ç²¾åº¦è®¡ç®— Prompt é•¿åº¦ ---
        # æˆ‘ä»¬ä¸ç›´æ¥ encode(full_prompt)ï¼Œè€Œæ˜¯é€šè¿‡å¯»æ‰¾ target çš„èµ·å§‹ token æ¥ç¡®å®š
        target_ids = self.tokenizer.encode(target_answer, add_special_tokens=False)
        
        # å¯»æ‰¾åˆ†ç•Œç‚¹ï¼šåœ¨ input_ids ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸å±äº prompt çš„ä½ç½®
        # æˆ‘ä»¬å¯ä»¥å…ˆ encode ä¸€ä¸ªå®Œå…¨æ²¡å¸¦ç‰¹æ®Šå­—ç¬¦çš„ prompt
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)

        labels = input_ids.clone()
        
        # å±è”½ Promptï¼šç¡®ä¿ä¸ä¼šè¶Šç•Œ
        safe_prompt_len = min(actual_prompt_len, self.max_length - 1)
        labels[:safe_prompt_len] = -100
        
        # å±è”½ Padding
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        # --- å±è”½ç‰¹æ®Š Token (ä¿ç•™ EOS å’Œ <|im_end|>) ---
        # è·å– <|im_end|> çš„ token IDï¼Œç¡®ä¿å®ƒè¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­
        im_end_token = "<|im_end|>"
        im_end_id = None
        try:
            # å°è¯•è·å– <|im_end|> çš„ token ID
            im_end_ids = self.tokenizer.encode(im_end_token, add_special_tokens=False)
            if im_end_ids:
                im_end_id = im_end_ids[0]  # é€šå¸¸ <|im_end|> æ˜¯ä¸€ä¸ªå•ç‹¬çš„ token
                # è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°ï¼‰
                if not hasattr(self, '_im_end_logged'):
                    print(f"âœ“ <|im_end|> token ID: {im_end_id}ï¼Œå°†è¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                    self._im_end_logged = True
        except Exception as e:
            if not hasattr(self, '_im_end_error_logged'):
                print(f"è­¦å‘Š: æ— æ³•è·å– <|im_end|> token ID: {e}")
                self._im_end_error_logged = True
        
        special_ids = set(self.tokenizer.all_special_ids)
        eos_id = self.tokenizer.eos_token_id
        # ä¿ç•™ EOS å’Œ <|im_end|> tokenï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢
        tokens_to_keep = {eos_id}
        if im_end_id is not None:
            tokens_to_keep.add(im_end_id)
        
        for tid in special_ids:
            if tid not in tokens_to_keep:
                labels[labels == tid] = -100
        
        # éªŒè¯ <|im_end|> æ˜¯å¦åœ¨ labels ä¸­ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if im_end_id is not None and (labels == im_end_id).any():
            if not hasattr(self, '_im_end_verified'):
                print(f"âœ“ ç¡®è®¤: <|im_end|> token (ID: {im_end_id}) å·²åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                self._im_end_verified = True

        # 4. æœ€ç»ˆéªŒè¯ï¼šé˜²æ­¢ NaN
        if (labels != -100).sum() == 0:
            # æŒ½æ•‘é€»è¾‘ï¼šå¦‚æœå…¨è¢«å±è”½äº†ï¼ˆè¯´æ˜æˆªæ–­å¤ªä¸¥é‡ï¼‰ï¼Œå¼ºè¡Œæš´éœ²æœ€å 32 ä¸ª token 
            # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ç­”æ¡ˆæé•¿æˆ–æˆªæ–­åˆšå¥½åˆ‡åœ¨äº†ç­”æ¡ˆå¼€å¤´
            labels[-32:] = input_ids[-32:]
            labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }


class CustomTrainer(Trainer):
    """å¸¦å®æ—¶æ—¥å¿—çš„è‡ªå®šä¹‰è®­ç»ƒå™¨"""
    
    def __init__(self, *args, verbose_logging=False, log_file_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.verbose_logging = verbose_logging
        self.log_file_path = log_file_path
        self.log_entry_count = 0
        
        if self.log_file_path:
            os.makedirs(os.path.dirname(self.log_file_path), exist_ok=True)
            self.log_file = open(self.log_file_path, 'w', encoding='utf-8')
            self.log_file.write("[\n")

    def __del__(self):
        if hasattr(self, 'log_file') and self.log_file:
            try:
                self.log_file.write("\n]")
                self.log_file.close()
            except: pass

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        outputs = model(**inputs)
        loss = outputs.loss if hasattr(outputs, 'loss') else None
        
        if loss is None and "labels" in inputs:
            logits = outputs.get("logits")
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["labels"][..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        if self.verbose_logging and (self.state.global_step % self.args.logging_steps == 0):
            self._log_details(inputs, outputs, loss.item())

        return (loss, outputs) if return_outputs else loss

    def clean_output_text(self, text: str) -> str:
        # ç§»é™¤æ€è€ƒè¿‡ç¨‹
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = text.replace('<think>', '').replace('</think>', '')
        return text.strip()

    def _log_details(self, inputs, outputs, loss_val):
        """è®°å½•è®­ç»ƒç»†èŠ‚ï¼šå¯¹æ¯” Target å’Œæ¨¡å‹çš„é¢„æµ‹ (Argmax)"""
        try:
            batch_idx = 0
            ids = inputs['input_ids'][batch_idx]
            lbs = inputs['labels'][batch_idx]
            logits = outputs.get("logits")[batch_idx]
            
            # è§£ç  Target
            target_ids = [t.item() for t in lbs if t != -100]
            target_text = self.tokenizer.decode(target_ids, skip_special_tokens=True)
            
            # è§£ç é¢„æµ‹ (å¯»æ‰¾ label æœ‰æ•ˆä½å¯¹åº”çš„é¢„æµ‹ä½)
            pred_ids_all = logits.argmax(dim=-1)
            valid_pos = (lbs != -100).nonzero(as_tuple=True)[0]
            pred_ids = [pred_ids_all[p-1].item() for p in valid_pos if p > 0]
            predict_text = self.tokenizer.decode(pred_ids, skip_special_tokens=True)
            
            print(f"\n[Step {self.state.global_step}] Loss: {loss_val:.4f}")
            print(f"ğŸ¯ Target: {target_text[:100]}")
            print(f"ğŸ¤– Predict: {predict_text[:100]}")

            if hasattr(self, 'log_file'):
                log_data = {
                    "step": self.state.global_step,
                    "loss": loss_val,
                    "target": target_text,
                    "predict": predict_text
                }
                if self.log_entry_count > 0: self.log_file.write(",\n")
                self.log_file.write(json.dumps(log_data, ensure_ascii=False))
                self.log_file.flush()
                self.log_entry_count += 1
        except Exception as e:
            print(f"Log Error: {e}")


class AblationTrainerDeepSpeed:
    """æ¶ˆèå®éªŒä¸»æ§ç±» - DeepSpeed ZeRO-3 å¤š GPU ç‰ˆæœ¬ï¼ˆä½¿ç”¨ Accelerate åº“ï¼‰"""
    
    def __init__(self, model_path: str, output_dir: str, config: Dict[str, Any], 
                 use_profile: bool = True, use_history: bool = True, use_context: bool = True, 
                 log_file_path: Optional[str] = None, deepspeed_config_path: Optional[str] = None):
        self.model_path = model_path
        self.output_dir = output_dir
        self.config = config
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        self.log_file_path = log_file_path

        # 1. åˆ›å»ºæˆ–ä½¿ç”¨æä¾›çš„ DeepSpeed é…ç½®æ–‡ä»¶
        # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åˆ›å»ºé»˜è®¤é…ç½®
        if deepspeed_config_path and os.path.exists(deepspeed_config_path):
            self.deepspeed_config_path = deepspeed_config_path
            print(f"âœ“ ä½¿ç”¨æä¾›çš„ DeepSpeed é…ç½®æ–‡ä»¶: {self.deepspeed_config_path}")
        else:
            self.deepspeed_config_path = self._create_deepspeed_config()
            print(f"âœ“ åˆ›å»º DeepSpeed é…ç½®æ–‡ä»¶: {self.deepspeed_config_path}")
        
        # 2. åŠ è½½ Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        # ç¡®ä¿ tokenizer æœ‰æ­£ç¡®çš„ padding side
        if not hasattr(self.tokenizer, 'padding_side') or self.tokenizer.padding_side is None:
            self.tokenizer.padding_side = "right"
        print(f"âœ“ Tokenizer è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        
        # 3. æ¨¡å‹å°†åœ¨è®­ç»ƒæ—¶é€šè¿‡ Trainer å’Œ DeepSpeed è‡ªåŠ¨åŠ è½½
        # Accelerate å’Œ Trainer ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰ DeepSpeed ç›¸å…³çš„åˆå§‹åŒ–
        print("âœ“ æ¨¡å‹å°†åœ¨è®­ç»ƒæ—¶é€šè¿‡ Accelerate + DeepSpeed è‡ªåŠ¨åŠ è½½")

    def _create_deepspeed_config(self) -> str:
        """åˆ›å»º DeepSpeed ZeRO-3 é…ç½®æ–‡ä»¶"""
        config_dir = Path(self.output_dir) / "deepspeed_config"
        config_dir.mkdir(parents=True, exist_ok=True)
        config_path = config_dir / "ds_config_zero3.json"
        
        # DeepSpeed ZeRO-3 é…ç½® - ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
        deepspeed_config = {
            "train_batch_size": "auto",
            "train_micro_batch_size_per_gpu": "auto",
            "gradient_accumulation_steps": "auto",
            "gradient_clipping": 1.0,
            "zero_optimization": {
                "stage": 3,
                # CPU Offload ä¼˜åŒ–å™¨å’Œå‚æ•°ä»¥èŠ‚çœæ˜¾å­˜
                "offload_optimizer": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "offload_param": {
                    "device": "cpu",
                    "pin_memory": True
                },
                "overlap_comm": True,
                "contiguous_gradients": True,
                "sub_group_size": 1e9,
                "reduce_bucket_size": "auto",
                "stage3_prefetch_bucket_size": "auto",
                "stage3_param_persistence_threshold": "auto",
                "stage3_max_live_parameters": 1e9,
                "stage3_max_reuse_distance": 1e9,
                "stage3_gather_16bit_weights_on_model_save": True,
                "round_robin_gradients": True
            },
            "bf16": {
                "enabled": True
            },
            "fp16": {
                "enabled": False
            },
            "optimizer": {
                "type": "AdamW",
                "params": {
                    "lr": "auto",
                    "betas": "auto",
                    "eps": "auto",
                    "weight_decay": "auto"
                }
            },
            "scheduler": {
                "type": "WarmupLR",
                "params": {
                    "warmup_min_lr": "auto",
                    "warmup_max_lr": "auto",
                    "warmup_num_steps": "auto"
                }
            },
            "wall_clock_breakdown": False,
            "steps_per_print": 10,
            # æ˜¾å­˜ä¼˜åŒ–
            "activation_checkpointing": {
                "partition_activations": True,
                "cpu_checkpointing": True,
                "contiguous_memory_optimization": True,
                "number_checkpoints": None,
                "synchronize_checkpoint_boundary": True,
                "profile": False
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(deepspeed_config, f, indent=2, ensure_ascii=False)
        
        return str(config_path)

    def train(self, train_samples: List[Dict[str, Any]], val_samples: Optional[List[Dict[str, Any]]] = None):
        import torch
        import os
        import transformers.models.gemma3.modeling_gemma3 as gemma3_module
        from transformers import TrainingArguments, AutoConfig, AutoModelForCausalLM
        
        # å°è¯•å¯¼å…¥ no_init_weightsï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
        try:
            from transformers.integrations import no_init_weights
            NO_INIT_WEIGHTS_AVAILABLE = True
        except ImportError:
            try:
                from accelerate import init_empty_weights
                NO_INIT_WEIGHTS_AVAILABLE = False
                print("âš  transformers.integrations.no_init_weights ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ accelerate.init_empty_weights")
            except ImportError:
                NO_INIT_WEIGHTS_AVAILABLE = False
                print("âš  no_init_weights å’Œ init_empty_weights éƒ½ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†åŠ è½½æ–¹å¼")

        train_config = self.config.get('training', {})
        
        # 1. åºåˆ—é•¿åº¦ä¸æ˜¾å­˜é¢„åˆ¤
        max_length = train_config.get('max_length', 4096)
        if max_length > 2048:
            print(f"âš  åºåˆ—é•¿åº¦ {max_length} è¾ƒå¤§ã€‚åœ¨ H100 ä¸Š ZeRO-3 è™½ç„¶èƒ½è·‘ï¼Œä½†ä¼šæ˜¾è‘—é™ä½ååã€‚")
        
        train_dataset = AblationDataset(
            train_samples, self.tokenizer, 
            max_length=max_length,
            use_profile=self.use_profile, use_history=self.use_history, use_context=self.use_context
        )

        # 2. è®­ç»ƒå‚æ•°ä¼˜åŒ–ï¼šé’ˆå¯¹ 27B + H100
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=train_config.get('num_epochs', 3),
            per_device_train_batch_size=train_config.get('batch_size', 1),
            gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 4),
            learning_rate=train_config.get('learning_rate', 2e-5), # 27B å…¨é‡å¾®è°ƒå»ºè®®è°ƒä½ LR
            logging_steps=train_config.get('logging_steps', 10),
            save_steps=train_config.get('save_steps', 500),
            save_total_limit=train_config.get('save_total_limit', 3),
            deepspeed=self.deepspeed_config_path,
            bf16=True, 
            gradient_checkpointing=True,
            # é’ˆå¯¹ H100 çš„ç‰¹æ®Šä¼˜åŒ–
            gradient_checkpointing_kwargs={"use_reentrant": False}, # é¿å…æŸäº›ç¯å¢ƒä¸‹çš„æ¢¯åº¦è­¦å‘Š
            warmup_steps=train_config.get('warmup_steps', 100),
            weight_decay=0.01,
            report_to="none",
            dataloader_num_workers=4, # H100 èŠ‚ç‚¹ CPU æ€§èƒ½é€šå¸¸è¾ƒå¼ºï¼Œå¯ä»¥ç•¥å¾®è°ƒé«˜
            remove_unused_columns=False,
        )

        # 3. å®šä¹‰æœ€æ ¸å¿ƒçš„æ¨¡å‹åˆå§‹åŒ–å‡½æ•° (ä¿®å¤ IndexError çš„å…³é”®)
        def model_init():
            print(">>> æ­£åœ¨å¯åŠ¨ ZeRO-3 å…¼å®¹æ€§åˆå§‹åŒ–æµç¨‹...")
            
            # --- æ ¸å¿ƒä¿®å¤ï¼šå¯¹ Gemma 3 ç±»çš„ _init_weights è¿›è¡Œå…¨å±€ Patch ---
            # ç†ç”±ï¼šZeRO-3 ä¼šåˆ†ç‰‡å‚æ•°ï¼Œå¯¼è‡´æŸäº›å¡ä¸Šçš„æœ¬åœ° weight ä¸ºç©ºï¼Œ
            # åŸç”Ÿä»£ç ä¸­ .zero_() è®¿é—® index 0 å°±ä¼šæŠ¥é”™ã€‚
            
            # å…³é”®ä¿®å¤ï¼šéœ€è¦ patch çˆ¶ç±» PreTrainedModel çš„ _init_weights æ–¹æ³•
            # å› ä¸º Gemma3 çš„ _init_weights ä¼šè°ƒç”¨ super()._init_weights(module)
            from transformers.modeling_utils import PreTrainedModel
            
            # ä¿å­˜åŸå§‹çš„çˆ¶ç±» _init_weights æ–¹æ³•
            original_parent_init_weights = PreTrainedModel._init_weights
            
            def safe_parent_init_weights(self, module):
                """å®‰å…¨çš„çˆ¶ç±»æƒé‡åˆå§‹åŒ–ï¼Œé¿å…åœ¨ DeepSpeed ZeRO-3 ç¯å¢ƒä¸‹å‡ºé”™"""
                # åªæœ‰å½“ weight ç¡®å®åœ¨æœ¬åœ°æ˜¾å­˜ä¸­æœ‰å…ƒç´ æ—¶ï¼Œæ‰æ‰§è¡Œåˆå§‹åŒ–
                if isinstance(module, torch.nn.Embedding):
                    # åœ¨ DeepSpeed ZeRO-3 ç¯å¢ƒä¸‹ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„æ£€æŸ¥
                    try:
                        # æ£€æŸ¥ weight æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•°æ®
                        if hasattr(module, 'weight') and module.weight is not None:
                            # å°è¯•è·å– weight çš„å½¢çŠ¶
                            weight_shape = module.weight.shape
                            if len(weight_shape) > 0 and weight_shape[0] > 0:
                                if module.padding_idx is not None:
                                    # å†æ¬¡æ£€æŸ¥ padding_idx æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                                    if 0 <= module.padding_idx < weight_shape[0]:
                                        module.weight.data[module.padding_idx].zero_()
                    except (IndexError, RuntimeError, AttributeError) as e:
                        # åœ¨ DeepSpeed ZeRO-3 ç¯å¢ƒä¸‹ï¼ŒæŸäº›å¡ä¸Šçš„æƒé‡å¯èƒ½ä¸ºç©ºæˆ–åˆ†ç‰‡
                        # è¿™æ˜¯æ­£å¸¸çš„ï¼Œç›´æ¥è·³è¿‡
                        pass
                    return  # è·³è¿‡è¯¥å±‚çš„é»˜è®¤åˆå§‹åŒ–
                # å¯¹äºå…¶ä»–å±‚ï¼Œè°ƒç”¨åŸå§‹æ–¹æ³•
                original_parent_init_weights(self, module)
            
            # Patch çˆ¶ç±»æ–¹æ³•
            PreTrainedModel._init_weights = safe_parent_init_weights
            print("âœ“ å·² patch PreTrainedModel._init_weights (çˆ¶ç±»æ–¹æ³•)")
            
            # åŒæ—¶ patch Gemma3 çš„ _init_weights æ–¹æ³•ï¼ˆè™½ç„¶å®ƒè°ƒç”¨ superï¼Œä½†ä¸ºäº†ä¿é™©ï¼‰
            def safe_init_weights_patch(self, module):
                # Gemma3 çš„ _init_weights ä¼šè°ƒç”¨ super()._init_weights(module)
                # æˆ‘ä»¬å·²ç» patch äº†çˆ¶ç±»æ–¹æ³•ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥è°ƒç”¨çˆ¶ç±»æ–¹æ³•å³å¯
                safe_parent_init_weights(self, module)

            # æ‰¾åˆ°ç›®æ ‡ç±»å¹¶æ›¿æ¢æ–¹æ³•
            target_classes = []
            if hasattr(gemma3_module, 'Gemma3ForCausalLM'):
                target_classes.append(gemma3_module.Gemma3ForCausalLM)
            if hasattr(gemma3_module, 'Gemma3ForConditionalGeneration'):
                target_classes.append(gemma3_module.Gemma3ForConditionalGeneration)
            if hasattr(gemma3_module, 'Gemma3Model'):
                target_classes.append(gemma3_module.Gemma3Model)
            
            patched_count = 0
            for cls in target_classes:
                if hasattr(cls, "_init_weights"):
                    cls._init_weights = safe_init_weights_patch
                    patched_count += 1
            
            print(f"âœ“ å·²æ³¨å…¥å®‰å…¨æƒé‡åˆå§‹åŒ–è¡¥ä¸ (Monkey Patch) - å·² patch {patched_count} ä¸ª Gemma3 ç±»")

            # --- ä½¿ç”¨ no_init_weights ä¸Šä¸‹æ–‡åŠ è½½æ¨¡å‹ ---
            # æ³¨æ„ï¼šno_init_weights ä¼šç¦ç”¨æƒé‡åˆå§‹åŒ–ï¼Œç›´æ¥ä» checkpoint åŠ è½½
            config = AutoConfig.from_pretrained(self.model_path, trust_remote_code=True)
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "config": config,
                "trust_remote_code": True,
                "torch_dtype": torch.bfloat16,
                "low_cpu_mem_usage": True,
            }

            # è‡ªåŠ¨æ¢æµ‹å¹¶å¼€å¯ Flash Attention 2
            try:
                import flash_attn
                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("âœ“ æ£€æµ‹åˆ° Flash Attention 2ï¼Œå·²å¼€å¯åŠ é€Ÿã€‚")
            except ImportError:
                print("âš  Flash Attention 2 æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›å®ç°")
            except Exception as e:
                print(f"âš  æ— æ³•å¯ç”¨ Flash Attention 2: {e}")

            # æ ¹æ®å¯ç”¨çš„å·¥å…·é€‰æ‹©åŠ è½½æ–¹å¼
            if NO_INIT_WEIGHTS_AVAILABLE:
                try:
                    with no_init_weights():
                        model = AutoModelForCausalLM.from_pretrained(
                            self.model_path,
                            **model_kwargs
                        )
                except Exception as e:
                    print(f"âš  ä½¿ç”¨ no_init_weights åŠ è½½å¤±è´¥: {e}")
                    print("å›é€€åˆ°æ ‡å‡†åŠ è½½æ–¹å¼ï¼ˆå·² patch _init_weightsï¼‰...")
                    model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        **model_kwargs
                    )
            else:
                # ä½¿ç”¨æ ‡å‡†åŠ è½½æ–¹å¼ï¼Œä½†å·²ç» patch äº† _init_weightsï¼Œåº”è¯¥èƒ½å·¥ä½œ
                print("ä½¿ç”¨æ ‡å‡†åŠ è½½æ–¹å¼ï¼ˆå·² patch _init_weights ä»¥å¤„ç† ZeRO-3ï¼‰...")
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    **model_kwargs
                )
            
            return model

        # 4. åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå™¨
        print(">>> æ­£åœ¨æ„å»º Trainer (ZeRO-3 å·²æŒ‚è½½)...")
        trainer = CustomTrainer(
            model_init=model_init, # é€šè¿‡ model_init å»¶è¿ŸåŠ è½½
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            verbose_logging=True,
            log_file_path=self.log_file_path
        )

        print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨: æ¶ˆèé…ç½® -> Profile={self.use_profile}, History={self.use_history}")
        trainer.train()
        
        # 5. ä¿å­˜ç»“æœï¼ˆRank 0 ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
        trainer.save_model(self.output_dir)
        print(f"âœ“ è®­ç»ƒåœ†æ»¡å®Œæˆã€‚æ¨¡å‹å·²å¯¼å‡ºè‡³: {self.output_dir}")