"""
è®­ç»ƒå™¨æ¨¡å— - ä¼˜åŒ–ç‰ˆ
é€‚é…æ¶ˆèå®éªŒï¼Œæ”¯æŒä¸¥æ ¼çš„è§’è‰²æ§åˆ¶ä¸æ—¥å¿—ç›‘æ§
"""
import os
import re
import time
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)
from typing import List, Dict, Any, Optional, Tuple
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ prompt_builder
sys.path.insert(0, str(Path(__file__).parent))
from prompt_builder import build_training_prompt


class AblationDataset(Dataset):
    def __init__(self, samples, tokenizer, max_length=32768, use_profile=True, use_history=True, use_context=True):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. åˆå§‹æ„å»º
        messages, target_answer = build_training_prompt(
            context=sample['context'],
            next_question=sample['next_question'],
            user_profile=sample.get('user_profile') if self.use_profile else None,
            task_description=sample.get('task_description'),
            history=sample.get('history') if self.use_history else None,
            use_profile=self.use_profile,
            use_history=self.use_history,
            use_context=self.use_context
        )

        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šåŠ¨æ€è£å‰ªå†å²ä»¥é˜²æ­¢æˆªæ–­ ---
        # å¦‚æœæ¶ˆæ¯å¤ªé•¿ï¼Œå¾ªç¯åˆ é™¤ messages ä¸­æœ€æ—©çš„å¯¹è¯è½®æ¬¡ï¼ˆä¿ç•™ system æç¤ºè¯ï¼‰
        # ç´¢å¼• 0 æ˜¯ systemï¼Œ1 å’Œ 2 æ˜¯æœ€æ—©çš„ä¸€å¯¹ user/assistant
        while len(self.tokenizer.apply_chat_template(messages, tokenize=True)) > (self.max_length - 512):
            if len(messages) > 2:
                messages.pop(1) # å¼¹å‡ºæœ€æ—©çš„å¯¹è¯
            else:
                break

        # 2. ç”Ÿæˆ Prompt (æ‰‹åŠ¨æ·»åŠ å¼•å¯¼ç¬¦)
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        generation_suffix = "<|im_start|>user\n"

        # 3. ç»„åˆæˆçœŸæ­£çš„ Prompt
        full_prompt = full_prompt.strip() + generation_suffix
        # ç¡®ä¿ä¸åŒ…å«ç­”æ¡ˆï¼Œä½¿ç”¨ <|im_end|> ä½œä¸ºç»“æŸæ ‡è®°ï¼ˆè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢ï¼‰
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token

        # 3. ç¼–ç 
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()

        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šé«˜ç²¾åº¦è®¡ç®— Prompt é•¿åº¦ ---
        # æˆ‘ä»¬ä¸ç›´æ¥ encode(full_prompt)ï¼Œè€Œæ˜¯é€šè¿‡å¯»æ‰¾ target çš„èµ·å§‹ token æ¥ç¡®å®š
        target_ids = self.tokenizer.encode(target_answer, add_special_tokens=False)
        
        # å¯»æ‰¾åˆ†ç•Œç‚¹ï¼šåœ¨ input_ids ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸å±äº prompt çš„ä½ç½®
        # æˆ‘ä»¬å¯ä»¥å…ˆ encode ä¸€ä¸ªå®Œå…¨æ²¡å¸¦ç‰¹æ®Šå­—ç¬¦çš„ prompt
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)

        labels = input_ids.clone()
        
        # å±è”½ Promptï¼šç¡®ä¿ä¸ä¼šè¶Šç•Œ
        safe_prompt_len = min(actual_prompt_len, self.max_length - 1)
        labels[:safe_prompt_len] = -100
        
        # å±è”½ Padding
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        # --- å±è”½ç‰¹æ®Š Token (ä¿ç•™ EOS å’Œ <|im_end|>) ---
        # è·å– <|im_end|> çš„ token IDï¼Œç¡®ä¿å®ƒè¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­
        im_end_token = "<|im_end|>"
        im_end_id = None
        try:
            # å°è¯•è·å– <|im_end|> çš„ token ID
            im_end_ids = self.tokenizer.encode(im_end_token, add_special_tokens=False)
            if im_end_ids:
                im_end_id = im_end_ids[0]  # é€šå¸¸ <|im_end|> æ˜¯ä¸€ä¸ªå•ç‹¬çš„ token
                # è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°ï¼‰
                if not hasattr(self, '_im_end_logged'):
                    print(f"âœ“ <|im_end|> token ID: {im_end_id}ï¼Œå°†è¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                    self._im_end_logged = True
        except Exception as e:
            if not hasattr(self, '_im_end_error_logged'):
                print(f"è­¦å‘Š: æ— æ³•è·å– <|im_end|> token ID: {e}")
                self._im_end_error_logged = True
        
        special_ids = set(self.tokenizer.all_special_ids)
        eos_id = self.tokenizer.eos_token_id
        # ä¿ç•™ EOS å’Œ <|im_end|> tokenï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢
        tokens_to_keep = {eos_id}
        if im_end_id is not None:
            tokens_to_keep.add(im_end_id)
        
        for tid in special_ids:
            if tid not in tokens_to_keep:
                labels[labels == tid] = -100
        
        # éªŒè¯ <|im_end|> æ˜¯å¦åœ¨ labels ä¸­ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if im_end_id is not None and (labels == im_end_id).any():
            if not hasattr(self, '_im_end_verified'):
                print(f"âœ“ ç¡®è®¤: <|im_end|> token (ID: {im_end_id}) å·²åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                self._im_end_verified = True

        # 4. æœ€ç»ˆéªŒè¯ï¼šé˜²æ­¢ NaN
        if (labels != -100).sum() == 0:
            # æŒ½æ•‘é€»è¾‘ï¼šå¦‚æœå…¨è¢«å±è”½äº†ï¼ˆè¯´æ˜æˆªæ–­å¤ªä¸¥é‡ï¼‰ï¼Œå¼ºè¡Œæš´éœ²æœ€å 32 ä¸ª token 
            # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ç­”æ¡ˆæé•¿æˆ–æˆªæ–­åˆšå¥½åˆ‡åœ¨äº†ç­”æ¡ˆå¼€å¤´
            labels[-32:] = input_ids[-32:]
            labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }


class CustomTrainer(Trainer):
    """å¸¦å®æ—¶æ—¥å¿—çš„è‡ªå®šä¹‰è®­ç»ƒå™¨"""
    
    def __init__(self, *args, verbose_logging=False, log_file_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.verbose_logging = verbose_logging
        self.log_file_path = log_file_path
        self.log_entry_count = 0
        
        if self.log_file_path:
            os.makedirs(os.path.dirname(self.log_file_path), exist_ok=True)
            self.log_file = open(self.log_file_path, 'w', encoding='utf-8')
            self.log_file.write("[\n")

    def __del__(self):
        if hasattr(self, 'log_file') and self.log_file:
            try:
                self.log_file.write("\n]")
                self.log_file.close()
            except: pass

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        outputs = model(**inputs)
        loss = outputs.loss if hasattr(outputs, 'loss') else None
        
        if loss is None and "labels" in inputs:
            logits = outputs.get("logits")
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["labels"][..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        if self.verbose_logging and (self.state.global_step % self.args.logging_steps == 0):
            self._log_details(inputs, outputs, loss.item())

        return (loss, outputs) if return_outputs else loss

    def clean_output_text(self, text: str) -> str:
        # ç§»é™¤æ€è€ƒè¿‡ç¨‹
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = text.replace('<think>', '').replace('</think>', '')
        return text.strip()

    def _log_details(self, inputs, outputs, loss_val):
        """è®°å½•è®­ç»ƒç»†èŠ‚ï¼šå¯¹æ¯” Target å’Œæ¨¡å‹çš„é¢„æµ‹ (Argmax)"""
        try:
            batch_idx = 0
            ids = inputs['input_ids'][batch_idx]
            lbs = inputs['labels'][batch_idx]
            logits = outputs.get("logits")[batch_idx]
            
            # è§£ç  Target
            target_ids = [t.item() for t in lbs if t != -100]
            target_text = self.tokenizer.decode(target_ids, skip_special_tokens=True)
            
            # è§£ç é¢„æµ‹ (å¯»æ‰¾ label æœ‰æ•ˆä½å¯¹åº”çš„é¢„æµ‹ä½)
            pred_ids_all = logits.argmax(dim=-1)
            valid_pos = (lbs != -100).nonzero(as_tuple=True)[0]
            pred_ids = [pred_ids_all[p-1].item() for p in valid_pos if p > 0]
            predict_text = self.tokenizer.decode(pred_ids, skip_special_tokens=True)
            
            print(f"\n[Step {self.state.global_step}] Loss: {loss_val:.4f}")
            print(f"ğŸ¯ Target: {target_text[:100]}")
            print(f"ğŸ¤– Predict: {predict_text[:100]}")

            if hasattr(self, 'log_file'):
                log_data = {
                    "step": self.state.global_step,
                    "loss": loss_val,
                    "target": target_text,
                    "predict": predict_text
                }
                if self.log_entry_count > 0: self.log_file.write(",\n")
                self.log_file.write(json.dumps(log_data, ensure_ascii=False))
                self.log_file.flush()
                self.log_entry_count += 1
        except Exception as e:
            print(f"Log Error: {e}")


class AblationTrainer:
    """æ¶ˆèå®éªŒä¸»æ§ç±»"""
    
    def __init__(self, model_path: str, output_dir: str, config: Dict[str, Any], 
                 use_profile: bool = True, use_history: bool = True, use_context: bool = True, log_file_path: Optional[str] = None):
        self.model_path = model_path
        self.output_dir = output_dir
        self.config = config
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        self.log_file_path = log_file_path

        # 1. åŠ è½½ Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 2. åŠ è½½æ¨¡å‹
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
            trust_remote_code=True
        ).to(self.device)
        
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()

    def train(self, train_samples: List[Dict[str, Any]], val_samples: Optional[List[Dict[str, Any]]] = None):
        train_config = self.config.get('training', {})
        
        train_dataset = AblationDataset(
            train_samples, self.tokenizer, 
            max_length=train_config.get('max_length', 32768),
            use_profile=self.use_profile, use_history=self.use_history, use_context=self.use_context
        )

        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=train_config.get('num_epochs', 3),
            per_device_train_batch_size=train_config.get('batch_size', 1),
            gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 16),
            learning_rate=train_config.get('learning_rate', 2e-5),
            logging_steps=10,
            save_steps=100,
            bf16=torch.cuda.is_bf16_supported(),
            fp16=not torch.cuda.is_bf16_supported(),
            report_to="none",
            remove_unused_columns=False
        )

        trainer = CustomTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            verbose_logging=True,
            log_file_path=self.log_file_path
        )

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: Profile={self.use_profile}, History={self.use_history}, Context={self.use_context}")
        trainer.train()
        
        # ä¿å­˜
        trainer.save_model(self.output_dir)
        self.tokenizer.save_pretrained(self.output_dir)