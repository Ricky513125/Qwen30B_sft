"""
æ¨ç†è„šæœ¬ï¼šä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆ test_leaderboard.json çš„ continuations
æ”¯æŒ6ä¸ªæ¶ˆèå®éªŒæ¨¡å‹ï¼ˆ3ç§é…ç½® Ã— 2ä¸ªæ•°æ®é›†ï¼‰
"""
import json
import argparse
import os
import sys
import re
import torch
from pathlib import Path
from tqdm import tqdm
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))
from data_loader import load_train_data, extract_training_samples, get_user_history_samples
from prompt_builder import build_prompt
from transformers import AutoTokenizer, AutoModelForCausalLM
try:
    from peft import PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("è­¦å‘Š: peft åº“æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½ LoRA æ¨¡å‹")

try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False
    print("è­¦å‘Š: deepspeed åº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ DeepSpeed ZeRO-3")


def load_test_leaderboard(test_leaderboard_path: str) -> list:
    """åŠ è½½ test_leaderboard.json"""
    with open(test_leaderboard_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data if isinstance(data, list) else [data]


def extract_context_from_leaderboard(sample: dict) -> list:
    """ä» leaderboard æ ·æœ¬ä¸­æå– context"""
    return sample.get('context', [])


def get_user_info_from_leaderboard(sample: dict, train_data: list) -> dict:
    """ä»è®­ç»ƒæ•°æ®ä¸­è·å–ç”¨æˆ·ä¿¡æ¯ï¼ˆprofile, historyç­‰ï¼‰"""
    # ä»æµ‹è¯•æ ·æœ¬ä¸­è·å–user_hash
    user_hash = sample.get('user_hash', '')
    if not user_hash:
        # å¦‚æœé¡¶å±‚æ²¡æœ‰ï¼Œå°è¯•ä»userå­—æ®µè·å–
        user = sample.get('user', {})
        # user_hashå¯èƒ½åœ¨userå­—æ®µä¸­ï¼Œæˆ–è€…éœ€è¦ä»å…¶ä»–å­—æ®µæ¨æ–­
    
    # ä»æµ‹è¯•æ ·æœ¬ä¸­è·å–user_profileï¼ˆIdealSelfåœºæ™¯ï¼‰
    user_profile = None
    user = sample.get('user', {})
    if user:
        user_profile = user.get('profile')
    
    # æ‰¾åˆ°è¯¥ç”¨æˆ·çš„è®­ç»ƒæ•°æ®
    user_train_samples = []
    if train_data:
        for item in train_data:
            if item.get('user_hash') == user_hash:
                user_train_samples.append(item)
    
    # å¦‚æœæµ‹è¯•æ ·æœ¬ä¸­æ²¡æœ‰profileï¼Œä»è®­ç»ƒæ•°æ®ä¸­è·å–
    if not user_profile and user_train_samples:
        first_sample = user_train_samples[0]
        train_user = first_sample.get('user', {})
        user_profile = train_user.get('profile')
    
    return {
        'user_hash': user_hash,
        'user_profile': user_profile,
        'user_train_samples': user_train_samples
    }

def generate_continuations(
    model,
    tokenizer,
    messages,
    num_samples=5,
    max_new_tokens=512,  # å¢åŠ åˆ°512ï¼Œç»™æ¨¡å‹è¶³å¤Ÿç©ºé—´å†™å®Œ"åºŸè¯"ï¼Œåç»­é€šè¿‡æ¸…æ´—å‡½æ•°æˆªæ–­
    device=None,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.2,  # å¢åŠ é‡å¤æƒ©ç½š
    no_repeat_ngram_size=4,  # å¢åŠ n-gramå¤§å°
    max_output_length=512,  # è¾“å‡ºæ–‡æœ¬æœ€å¤§å­—ç¬¦æ•°
    is_japanese_task=False,  # æ˜¯å¦ä¸ºæ—¥è¯­ä»»åŠ¡
):
    # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼šä½¿ç”¨ add_generation_prompt=Falseï¼Œç„¶åæ‰‹åŠ¨æ·»åŠ å¼•å¯¼ç¬¦
    # è®­ç»ƒæ—¶çš„é€»è¾‘ï¼šfull_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
    #                generation_suffix = "<|im_start|>user\n"
    #                full_prompt = full_prompt.strip() + generation_suffix
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
    )
    
    # æ‰‹åŠ¨æ·»åŠ å¼•å¯¼ç¬¦ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
    generation_suffix = "<|im_start|>user\n"
    text = text.strip() + generation_suffix
    
    # å¦‚æœæ˜¯æ—¥è¯­ä»»åŠ¡ï¼Œåœ¨æœ«å°¾æ·»åŠ æ—¥è¯­å¼•å¯¼è¯ï¼Œè¿«ä½¿æ¨¡å‹è¿›å…¥æ—¥è¯­è¯­å¢ƒ
    if is_japanese_task:
        # æ·»åŠ æ—¥è¯­å¼•å¯¼è¯ï¼Œå¸®åŠ©æ¨¡å‹ç›´æ¥è¿›å…¥æ—¥è¯­å›ç­”æ¨¡å¼
        japanese_prompts = [
            "å›ç­”ï¼š",
            "è¿”ç­”ï¼š",
            "å¿œç­”ï¼š",
        ]
        import random
        text = text.rstrip() + "\n" + random.choice(japanese_prompts)

    # å¯¹äºå¤š GPU æ¨¡å‹æˆ– DeepSpeed æ¨¡å‹ï¼Œå¦‚æœ device ä¸º Noneï¼Œè®©æ¨¡å‹è‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…
    # å¦åˆ™å°†è¾“å…¥æ”¾åˆ°æŒ‡å®šè®¾å¤‡
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )
    # å¦‚æœæŒ‡å®šäº† deviceï¼Œå°†è¾“å…¥ç§»åˆ°è¯¥è®¾å¤‡
    # å¯¹äºä½¿ç”¨ device_map æˆ– DeepSpeed çš„æ¨¡å‹ï¼Œå¯ä»¥ä¸ç§»åŠ¨ï¼Œè®©æ¨¡å‹è‡ªåŠ¨å¤„ç†
    # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œå¦‚æœæŒ‡å®šäº† deviceï¼Œä»ç„¶ç§»åŠ¨è¾“å…¥
    if device is not None:
        inputs = {k: v.to(device) for k, v in inputs.items()}
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®š deviceï¼Œå°è¯•ä»æ¨¡å‹è·å–ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨çš„è®¾å¤‡
        try:
            model_device = next(model.parameters()).device
            inputs = {k: v.to(model_device) for k, v in inputs.items()}
        except (StopIteration, AttributeError):
            # å¦‚æœæ— æ³•è·å–è®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
    
    # ç¡®ä¿ input_ids åœ¨æœ‰æ•ˆèŒƒå›´å†…
    model_vocab_size = model.config.vocab_size
    if inputs["input_ids"].max().item() >= model_vocab_size:
        print(f"  è­¦å‘Š: æ£€æµ‹åˆ° token ID è¶…å‡ºæ¨¡å‹è¯æ±‡è¡¨èŒƒå›´ï¼Œè¿›è¡Œè£å‰ª...")
        inputs["input_ids"] = torch.clamp(inputs["input_ids"], 0, model_vocab_size - 1)

    # è®¾ç½®ç”Ÿæˆå‚æ•°
    generation_kwargs = {
        "max_new_tokens": max_new_tokens,
        "do_sample": do_sample,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "repetition_penalty": repetition_penalty,  # é˜²æ­¢é‡å¤ï¼Œæ¨è 1.1-1.2
        "no_repeat_ngram_size": no_repeat_ngram_size,  # é˜²æ­¢ n-gram é‡å¤ï¼Œæ¨è 3-4
    }
    
    if do_sample:
        generation_kwargs["temperature"] = temperature  # 0.7-0.8 ä»¥å¢å¼ºé€»è¾‘ç¨³å®šæ€§
        generation_kwargs["top_p"] = top_p
        generation_kwargs["top_k"] = 50  # é™åˆ¶å€™é€‰ token æ•°é‡ï¼Œæé«˜è´¨é‡
    
    # å¦‚æœè¯æ±‡è¡¨ä¸åŒ¹é…ï¼Œä½¿ç”¨ LogitsProcessor é™åˆ¶è¯æ±‡è¡¨èŒƒå›´å¹¶æ¸…ç†æ— æ•ˆå€¼
    if hasattr(model.config, 'vocab_size') and tokenizer.vocab_size < model.config.vocab_size:
        from transformers import LogitsProcessor
        class VocabSizeLimiter(LogitsProcessor):
            def __init__(self, max_vocab_size):
                self.max_vocab_size = max_vocab_size
            
            def __call__(self, input_ids, scores):
                # å°†è¶…å‡ºèŒƒå›´çš„ logits è®¾ä¸ºè´Ÿæ— ç©·
                scores[:, self.max_vocab_size:] = float('-inf')
                
                # æ¸…ç†æ— æ•ˆå€¼ï¼šå°† inf å’Œ nan æ›¿æ¢ä¸ºè´Ÿæ— ç©·
                scores = torch.where(
                    torch.isnan(scores) | torch.isinf(scores),
                    torch.tensor(float('-inf'), device=scores.device, dtype=scores.dtype),
                    scores
                )
                
                # ç¡®ä¿æ²¡æœ‰æ­£æ— ç©·ï¼ˆä¼šå¯¼è‡´ softmax åæ¦‚ç‡ä¸º nanï¼‰
                scores = torch.clamp(scores, min=-1e10, max=1e10)
                
                return scores
        
        from transformers import LogitsProcessorList
        logits_processor = LogitsProcessorList([VocabSizeLimiter(tokenizer.vocab_size)])
        generation_kwargs["logits_processor"] = logits_processor
    else:
        # å³ä½¿è¯æ±‡è¡¨åŒ¹é…ï¼Œä¹Ÿæ·»åŠ æ¸…ç†æ— æ•ˆå€¼çš„å¤„ç†å™¨
        from transformers import LogitsProcessor
        class LogitsCleaner(LogitsProcessor):
            def __call__(self, input_ids, scores):
                # æ¸…ç†æ— æ•ˆå€¼ï¼šå°† inf å’Œ nan æ›¿æ¢ä¸ºè´Ÿæ— ç©·
                scores = torch.where(
                    torch.isnan(scores) | torch.isinf(scores),
                    torch.tensor(float('-inf'), device=scores.device, dtype=scores.dtype),
                    scores
                )
                # é™åˆ¶ logits èŒƒå›´ï¼Œé¿å…æ•°å€¼æº¢å‡º
                scores = torch.clamp(scores, min=-1e10, max=1e10)
                return scores
        
        # æ·»åŠ é‡å¤æƒ©ç½šå¤„ç†å™¨ï¼ˆé¢å¤–ä¿æŠ¤ï¼‰
        class RepetitionPenaltyProcessor(LogitsProcessor):
            """é¢å¤–çš„é‡å¤æƒ©ç½šå¤„ç†å™¨ï¼Œé˜²æ­¢ç”Ÿæˆé‡å¤å†…å®¹"""
            def __init__(self, penalty=1.1):
                self.penalty = penalty
            
            def __call__(self, input_ids, scores):
                # å¯¹æœ€è¿‘ç”Ÿæˆçš„ token åº”ç”¨é¢å¤–çš„æƒ©ç½š
                if len(input_ids[0]) > 0:
                    # è·å–æœ€è¿‘ 10 ä¸ª token
                    recent_tokens = input_ids[0][-10:].tolist()
                    for token_id in recent_tokens:
                        if token_id < scores.shape[-1]:
                            scores[0, token_id] = scores[0, token_id] / self.penalty
                return scores
        
        from transformers import LogitsProcessorList
        logits_processor = LogitsProcessorList([
            LogitsCleaner(),
            RepetitionPenaltyProcessor(penalty=1.1)  # é¢å¤–çš„é‡å¤æƒ©ç½š
        ])
        generation_kwargs["logits_processor"] = logits_processor

    try:
        # æ³¨æ„ï¼štransformersçš„generateæ–¹æ³•ä¸æ”¯æŒstop_stringså‚æ•°
        # åœæ­¢é€»è¾‘é€šè¿‡eos_token_idå’Œmax_new_tokensæ¥æ§åˆ¶
        # å¦‚æœéœ€è¦æ›´å¤æ‚çš„åœæ­¢è¯é€»è¾‘ï¼Œå¯ä»¥ä½¿ç”¨StoppingCriteriaï¼Œä½†è¿™é‡Œæš‚æ—¶ä¸éœ€è¦
        
        outputs = model.generate(
            **inputs,
            **generation_kwargs,
        )
        gen = outputs[0][inputs["input_ids"].shape[1]:]
        
        # æ¸…ç†ç”Ÿæˆçš„ token IDsï¼Œç¡®ä¿å®ƒä»¬åœ¨æœ‰æ•ˆèŒƒå›´å†…
        tokenizer_vocab_size = tokenizer.vocab_size
        
        # ç¡®ä¿ gen æ˜¯ tensor å¹¶ä¸”ç§»åˆ° CPU
        if isinstance(gen, torch.Tensor):
            gen = gen.cpu()
        else:
            gen = torch.tensor(gen, dtype=torch.long)
        
        # æ¸…ç†æ— æ•ˆçš„ token IDsï¼ˆè¶…å‡ºè¯æ±‡è¡¨èŒƒå›´ã€NaNã€Infç­‰ï¼‰
        # å°†æ— æ•ˆå€¼æ›¿æ¢ä¸º eos_token_id æˆ– pad_token_id
        valid_mask = (gen >= 0) & (gen < tokenizer_vocab_size) & torch.isfinite(gen.float())
        
        if valid_mask.sum() == 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆ tokenï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            return [""]
        
        # åªä¿ç•™æœ‰æ•ˆçš„ token IDs
        cleaned_gen = gen[valid_mask]
        
        # å¦‚æœæ¸…ç†åæ²¡æœ‰æœ‰æ•ˆ tokenï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if len(cleaned_gen) == 0:
            return [""]
        
        # è§£ç 
        try:
            decoded_text = tokenizer.decode(cleaned_gen, skip_special_tokens=True).strip()
            # decoded_text = tokenizer.decode(gen, skip_special_tokens=True).strip()
    
            # è°ƒè¯•ï¼šæ‰“å°åŸå§‹è§£ç æ–‡æœ¬
            if not decoded_text:
                print(f"  è­¦å‘Š: è§£ç åæ–‡æœ¬ä¸ºç©º")
                return [""]
            
            # --- è°ƒç”¨æ¸…æ´—å‡½æ•° ---
            cleaned_text = clean_model_output(decoded_text, max_length=max_output_length)
            
            # è°ƒè¯•ï¼šå¦‚æœæ¸…ç†åä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ï¼ˆè‡³å°‘ä¿ç•™ä¸€äº›å†…å®¹ï¼‰
            if not cleaned_text or len(cleaned_text.strip()) < 3:
                if decoded_text and len(decoded_text.strip()) > 0:
                    print(f"  è­¦å‘Š: æ¸…ç†åæ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œä¿ç•™åŸå§‹æ–‡æœ¬çš„å‰{min(max_output_length, len(decoded_text))}å­—ç¬¦")
                    # åªåšæœ€åŸºæœ¬çš„æ¸…ç†ï¼šç§»é™¤æ˜æ˜¾çš„åƒåœ¾token
                    fallback = decoded_text
                    for pattern in [r'\bVRTX\b', r'\bVERTEX\b', r'<Vertex>', r'\(Vertex\)', r'_VERTEX_',
                                   r'\bPorno\b', r'\bporno\b', r'Viagra\s+Porno']:
                        fallback = re.sub(pattern, '', fallback, flags=re.IGNORECASE)
                    cleaned_text = fallback[:max_output_length].strip() if fallback.strip() else ""
                else:
                    cleaned_text = ""
            
            return [cleaned_text if cleaned_text else ""]
        except Exception as decode_error:
            # å¦‚æœè§£ç ä»ç„¶å¤±è´¥ï¼Œå°è¯•é€ä¸ª token è§£ç 
            print(f"  è­¦å‘Š: æ‰¹é‡è§£ç å¤±è´¥ï¼Œå°è¯•é€ä¸ªè§£ç : {decode_error}")
            try:
                decoded_parts = []
                for token_id in cleaned_gen.tolist():
                    try:
                        token_text = tokenizer.decode([token_id], skip_special_tokens=True)
                        if token_text:
                            decoded_parts.append(token_text)
                    except:
                        continue
                decoded_text = "".join(decoded_parts).strip()
                return [decoded_text if decoded_text else ""]
            except:
                return [""]
    
    except Exception as e:
        # å¦‚æœè§£ç å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        error_msg = str(e)
        if "NoneType" in error_msg or "expected str instance" in error_msg:
            print(f"  è­¦å‘Š: è§£ç æ—¶é‡åˆ° None å€¼ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
            return [""]
        else:
            # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
            raise



def log_inference_details(
    log_file,
    sample_idx: int,
    data_item_idx: int,
    context: list,
    messages: list,
    user_info: dict,
    history_evidence: list,
    continuations: list,
    reference: str = None,
    error: str = None,
    is_first_entry: bool = False
):
    """è®°å½•æ¨ç†è¯¦æƒ…åˆ°æ—¥å¿—æ–‡ä»¶"""
    try:
        # æ„å»ºæ—¥å¿—æ¡ç›®
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "sample_idx": sample_idx,
            "data_item_idx": data_item_idx,
            "context": context,
            "input_prompt": {
                "messages": messages,
                "message_count": len(messages)
            },
            "user_info": {
                "user_hash": user_info.get('user_hash', ''),
                "has_profile": user_info.get('user_profile') is not None,
                "profile": user_info.get('user_profile'),
                "history_count": len(user_info.get('user_train_samples', []))
            },
            "history_evidence_count": len(history_evidence),
            "history_evidence": [
                {
                    "context": h.get('context', [])[:3] if h.get('context') else [],  # åªè®°å½•å‰3ä¸ªturn
                    "continuation": h.get('continuation', '')[:100] if h.get('continuation') else ''  # åªè®°å½•å‰100å­—ç¬¦
                }
                for h in history_evidence[:3]  # æœ€å¤š3ä¸ªå†å²æ ·æœ¬
            ],
            "outputs": {
                "continuations": continuations,
                "count": len(continuations)
            },
            "target": reference if reference else None,
            "error": error if error else None
        }
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        if not is_first_entry:
            log_file.write(",\n")  # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¡ï¼Œå…ˆå†™é€—å·
        log_file.write(json.dumps(log_entry, ensure_ascii=False, indent=2))
        log_file.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
        
    except Exception as e:
        print(f"  è­¦å‘Š: è®°å½•æ—¥å¿—æ—¶å‡ºé”™: {e}")


import re

def check_model_files(model_path: str) -> dict:
    """
    æ£€æŸ¥æ¨¡å‹ç›®å½•ä¸­çš„æ–‡ä»¶ç±»å‹
    
    Returns:
        dict with keys: 'has_safetensors', 'has_pytorch', 'safetensors_files', 'pytorch_files'
    """
    result = {
        'has_safetensors': False,
        'has_pytorch': False,
        'safetensors_files': [],
        'pytorch_files': []
    }
    
    if not os.path.exists(model_path):
        return result
    
    for file in os.listdir(model_path):
        if file.endswith('.safetensors'):
            result['has_safetensors'] = True
            result['safetensors_files'].append(file)
        elif file.endswith('.bin') and 'pytorch_model' in file:
            result['has_pytorch'] = True
            result['pytorch_files'].append(file)
    
    return result




def clean_model_output(text: str, max_length: int = 512) -> str:
    """
    ç²¾ç®€ä¼˜åŒ–ç‰ˆï¼šé€šè¿‡â€œå¼ºåŠ›æˆªæ–­â€é€»è¾‘ï¼Œåªä¿ç•™ç¬¬ä¸€æ®µæ ¸å¿ƒå¯¹è¯ã€‚
    é€‚ç”¨äºè§£å†³ï¼šæ¨¡å‹å¤è¯»ã€å¸¦æ‹¬å·çš„å¿ƒç†æè¿°ã€ä¸­æ–‡æ¨ç†æ±¡æŸ“æ—¥è¯­å›ç­”ç­‰é—®é¢˜ã€‚
    """
    if not text:
        return ""

    # 1. ç»“æ„åŒ–æ¸…ç†ï¼šç§»é™¤æ€è€ƒè¿‡ç¨‹å’Œæ‰€æœ‰ Chat æ¨¡æ¿æ ‡ç­¾
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    # ç§»é™¤ <|im_start|>, <|im_end|>, <|user|>, <|assistant|> ç­‰åŠå…¶åçš„æ¢è¡Œ
    text = re.sub(r'<\|im_start\|>.*?\n|<\|im_end\|>|<\|user\|>|<\|assistant\|>', '', text)
    text = text.replace('<think>', '').replace('</think>', '').replace('ROKE', '')

    # 2. å®šä¹‰â€œåœæ­¢ç‚¹â€æ ‡è¯†ç¬¦ (Stop Markers)
    # ä¸€æ—¦åœ¨æ¢è¡ŒååŒ¹é…åˆ°è¿™äº›æ¨¡å¼ï¼Œè¯´æ˜æ­£æ–‡ç»“æŸï¼Œå¼€å§‹è¾“å‡ºå…ƒæ•°æ®æˆ–åƒåœ¾å†…å®¹
    stop_markers = [
        r'\nï¼ˆ', r'\n\(',      # æ¢è¡Œåçš„æ‹¬å·ï¼ˆè§’è‰²/å¿ƒç†è¯´æ˜ï¼‰
        r'\n[*]{2,}',          # æ¢è¡Œåçš„è¿ç»­æ˜Ÿå·
        r'\nâ€»',                # æ¢è¡Œåçš„ç‰¹æ®Šç¬¦å·
        r'\nå•é¡Œ[ï¼š:]',         # è®­ç»ƒé¢˜ç›®å¤è¯»
        r'\næœ€ç»ˆç”Ÿæˆ',          # ä¸­æ–‡æ¨ç†æ ‡è¯†
        r'\nåˆ†æ',              # ä¸­æ–‡åˆ†æ
        r'\nå»ºè®®',              # ä¸­æ–‡å»ºè®®
        r'\n\s*---',           # åˆ†éš”çº¿
        r'\n[A-Z]\)',          # é€‰æ‹©é¢˜ A) B) æ¨¡å¼
        r'RealPersonaChat',    # åœºæ™¯åæ³„éœ²
    ]
    
    # æ‰§è¡Œå¼ºåŠ›æˆªæ–­ï¼šåªä¿ç•™ç¬¬ä¸€ä¸ªåœæ­¢ä½ä¹‹å‰çš„å†…å®¹
    combined_stop_pattern = '|'.join(stop_markers)
    match = re.search(combined_stop_pattern, text)
    if match:
        text = text[:match.start()]

    # 3. æå–â€œç¬¬ä¸€æ®µâ€æœ‰æ•ˆå†…å®¹
    # è¿‡æ»¤æ‰ç©ºç™½è¡Œï¼Œåªå–ç¬¬ä¸€è¡Œæˆ–ç¬¬ä¸€æ®µæœ‰æ„ä¹‰çš„æ–‡å­—
    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    if not paragraphs:
        return ""
    
    # æ‹¿åˆ°ç¬¬ä¸€æ®µè¯ä½œä¸ºæ ¸å¿ƒå›å¤
    result = paragraphs[0]

    # 4. è¡Œå†…æ¸…æ´—ï¼šå¤„ç† PersonaChat ç‰¹æœ‰çš„è¡Œå†…æ‹¬å·æ±¡æŸ“
    # æ¯”å¦‚ç§»é™¤æœ«å°¾çš„ "(ç¬‘é¡”ã§)" æˆ– "ï¼ˆå¬‰ã—ã„ï¼‰"
    result = re.sub(r'[ï¼ˆ\(][^ï¼‰\)]*?(æ„Ÿæƒ…|æ€åº¦|æ³¨|è§’è‰²|å§¿æ€|ç¬‘é¡”|å–œ)[^ï¼‰\)]*?[ï¼‰\)]', '', result)
    
    # 5. è¯­è¨€æ±¡æŸ“å¤„ç† (é’ˆå¯¹â€œä¸­æ–‡æ¨ç† + æ—¥è¯­å›ç­”â€çš„æƒ…å†µ)
    # å¦‚æœç¬¬ä¸€æ®µè¯é‡Œæœ‰â€œå›ç­”ï¼šâ€æˆ–â€œç­”ï¼šâ€ï¼Œåªå–å†’å·åçš„å†…å®¹
    for marker in ['å›ç­”ï¼š', 'è¿”ç­”ï¼š', 'å¿œç­”ï¼š', 'ç­”ï¼š', 'æœ€ç»ˆç”Ÿæˆï¼š']:
        if marker in result:
            result = result.split(marker)[-1]

    # 6. æè‡´æ ‡ç‚¹æ¸…ç† (ç§»é™¤æœ«å°¾å¤šä½™çš„ç¬¦å·)
    result = result.strip().rstrip('â€»-* \n')
    
    # 7. å…œåº•é€»è¾‘ï¼šå¦‚æœæ¸…ç†åå¤ªçŸ­è€ŒåŸå§‹æ–‡æœ¬å¾ˆé•¿ï¼Œè¯´æ˜å¯èƒ½è¯¯åˆ äº†ï¼Œä¿ç•™åŸå§‹æ–‡æœ¬å‰ä¸€æ®µ
    if len(result) < 2 and len(text) > 5:
        return text.strip()[:max_length]

    return result[:max_length].strip()


# def clean_model_output(text: str, max_length: int = 32768) -> str:
#     """æè‡´æ¸…ç†ï¼šç§»é™¤æ€è€ƒè¿‡ç¨‹ã€ChatML æ ‡ç­¾ã€è§’è‰²æ ‡è¯†ã€åƒåœ¾token"""
#     if not text:
#         return ""
    
#     # ä¿å­˜åŸå§‹æ–‡æœ¬ï¼Œä»¥é˜²æ¸…ç†åä¸ºç©º
#     original_text = text
    
#     # 1. ç§»é™¤ <think>...</think> åŠå…¶å†…éƒ¨çš„æ‰€æœ‰å†…å®¹
#     text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    
#     # 2. ç§»é™¤æ®‹ä½™çš„æˆ–æœªé—­åˆçš„æ ‡ç­¾
#     text = text.replace('<think>', '').replace('</think>', '')
    
#     # 3. ç§»é™¤åƒåœ¾tokenå’Œæ¨¡å¼ï¼ˆVRTX, Vertex, Porno, oplayer, optimizerç­‰ï¼‰
#     # æ³¨æ„ï¼šåªç§»é™¤æ˜ç¡®çš„åƒåœ¾tokenï¼Œä¸è¦è¯¯åˆ æ­£å¸¸å†…å®¹
#     garbage_patterns = [
#         r'\bVRTX\b', r'\bVERTEX\b', r'<Vertex>', r'\(Vertex\)', r'_VERTEX_',
#         r'\bPorno\b', r'\bporno\b', r'\bPorn\b', r'\bporn\b', r'\bXXX\b', r'\bxxx\b',
#         r'\boplayer\b', r'\boptimizer\b', r'\boyal\b',
#         r'Viagra\s+Porno', r'Porno\s+Porno',
#         r'è‰²æƒ…', r'æš´åŠ›', r'ææ€–', r'æˆäºº', r'æ€§çˆ±', r'æ¿€æƒ…', r'æƒ…è‰²',
#     ]
#     for pattern in garbage_patterns:
#         text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
#     # 4. ç§»é™¤æ¨¡å‹å¯èƒ½è¾“å‡ºçš„ meta-commentaryï¼ˆä¾‹å¦‚ï¼šå¥½çš„ï¼Œæˆ‘éœ€è¦åˆ†æ...ï¼‰
#     # è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼è§„åˆ™ï¼šå¦‚æœå›ç­”ä¸­åŒ…å«å¤§æ®µå…³äº"ç”¨æˆ·"ã€"å¯¹è¯"ã€"åˆ†æ"çš„ä¸­æ–‡è§£é‡Šï¼Œå°è¯•æˆªæ–­
#     # æ³¨æ„ï¼šè¿™ä¸ªé€»è¾‘å¯èƒ½è¿‡äºæ¿€è¿›ï¼Œåªåœ¨æ˜ç¡®æ‰¾åˆ°ç­”æ¡ˆæ—¶æ‰æˆªæ–­ï¼Œå¦åˆ™ä¿ç•™åŸæ–‡æœ¬
#     if "åˆ†æ" in text or "å»ºè®®" in text or "è¦æ±‚" in text:
#         # å¦‚æœæ¨¡å‹åœ¨æœ€åæ‰ç»™ç­”æ¡ˆï¼Œç­”æ¡ˆé€šå¸¸åœ¨ [[ ]] æˆ– "æœ€ç»ˆç”Ÿæˆï¼š" ä¹‹å
#         final_ans = re.search(r'\[\[(.*?)\]\]', text)
#         if final_ans and final_ans.group(1).strip():
#             text = final_ans.group(1)
#         elif "æœ€ç»ˆç”Ÿæˆ" in text:
#             parts = text.split("æœ€ç»ˆç”Ÿæˆ")
#             if len(parts) > 1 and parts[-1].strip():
#                 text = parts[-1]
#             # å¦‚æœåˆ†å‰²åä¸ºç©ºï¼Œä¿ç•™åŸæ–‡æœ¬ï¼ˆä¸åšä»»ä½•ä¿®æ”¹ï¼‰

#     # 5. ç§»é™¤ Chat æ¨¡æ¿ç›¸å…³çš„æ ‡è®°
#     special_patterns = [r'<\|im_start\|>.*?\n', r'<\|im_end\|>', r'<\|user\|>', r'<\|assistant\|>']
#     for pattern in special_patterns:
#         text = re.sub(pattern, '', text)
        
#     # 6. ç§»é™¤å¥‡æ€ªçš„é‡å¤åç¼€ (å¦‚ä½ ç»“æœä¸­çš„ ROKE)
#     text = text.replace('ROKE', '').strip()
    
#     # 6.5. ç§»é™¤é›¶å®½å­—ç¬¦å’Œç‰¹æ®ŠUnicodeæ§åˆ¶å­—ç¬¦
#     text = re.sub(r'[\u200b-\u200f\u202a-\u202e\u2060-\u206f\ufeff]', '', text)
#     text = re.sub(r'[\u2028\u2029]', '\n', text)
    
#     # 6.6. ä¼˜å…ˆåœ¨ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦å¤„æˆªæ–­ï¼ˆå¦‚æœåé¢è·Ÿç€åƒåœ¾å†…å®¹ï¼‰
#     first_newline_pos = text.find('\n')
#     if first_newline_pos > 0:
#         # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦åçš„å†…å®¹
#         after_first_line = text[first_newline_pos + 1:].strip()
#         # åƒåœ¾æ¨¡å¼ï¼šä»¥æ‹¬å·ã€ç¬¦å·ã€å…ƒæ•°æ®ç­‰å¼€å¤´
#         garbage_start_patterns = [
#             r'^ï¼ˆ',  # ä»¥"ï¼ˆ"å¼€å¤´ï¼ˆå¦‚"ï¼ˆæ„Ÿæƒ…ãƒ»æ…‹åº¦ï¼‰"ã€"ï¼ˆæ³¨ï¼š...ï¼‰"ï¼‰
#             r'^\(',  # ä»¥"("å¼€å¤´
#             r'^â€»',  # ä»¥"â€»"å¼€å¤´
#             r'^\*\*\*\*',  # ä»¥"****"å¼€å¤´
#             r'^\*\*',  # ä»¥"**"å¼€å¤´
#             r'^---',  # ä»¥"---"å¼€å¤´
#             r'^stage',  # ä»¥"stage"å¼€å¤´
#             r'^å•é¡Œ',  # ä»¥"å•é¡Œ"å¼€å¤´
#             r'^ï¼ˆä»¥ä¸‹',  # ä»¥"ï¼ˆä»¥ä¸‹"å¼€å¤´
#             r'^ï¼ˆæ³¨',  # ä»¥"ï¼ˆæ³¨"å¼€å¤´
#             r'^ï¼ˆä¸Šè¨˜',  # ä»¥"ï¼ˆä¸Šè¨˜"å¼€å¤´
#             r'^[A-Z]\)',  # ä»¥"A)"å¼€å¤´ï¼ˆé€‰æ‹©é¢˜ï¼‰
#         ]
#         for pattern in garbage_start_patterns:
#             if re.match(pattern, after_first_line, re.IGNORECASE):
#                 # åœ¨ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦å¤„æˆªæ–­
#                 text = text[:first_newline_pos].strip()
#                 break
    
#     # 6.7. æ£€æµ‹å¹¶æˆªæ–­å…¶ä»–åƒåœ¾æ¨¡å¼
#     garbage_markers = [
#         r'\nï¼ˆ',  # æ¢è¡Œåè·Ÿ"ï¼ˆ"
#         r'\n\(',  # æ¢è¡Œåè·Ÿ"("
#         r'\nâ€»',  # æ¢è¡Œåè·Ÿ"â€»"
#         r'\n\*\*\*\*',  # æ¢è¡Œåè·Ÿ"****"
#         r'\n\*\*[ï¼š:]',  # æ¢è¡Œåè·Ÿ"**:"
#         r'\nå•é¡Œ[ï¼š:]',  # æ¢è¡Œåè·Ÿ"å•é¡Œï¼š"
#         r'\nstage',  # æ¢è¡Œåè·Ÿ"stage"
#         r'\n---',  # æ¢è¡Œåè·Ÿ"---"
#         r'\n[A-Z]\)\s*[A-Z]\)',  # æ¢è¡Œåè·Ÿé€‰æ‹©é¢˜æ¨¡å¼
#     ]
#     first_garbage_pos = len(text)
#     for pattern in garbage_markers:
#         match = re.search(pattern, text, re.IGNORECASE)
#         if match:
#             pos = match.start()
#             if pos < first_garbage_pos:
#                 first_garbage_pos = pos
    
#     if first_garbage_pos < len(text):
#         text = text[:first_garbage_pos].strip()
    
#     # 6.8. ç§»é™¤æœ«å°¾çš„é‡å¤åƒåœ¾æ¨¡å¼
#     trailing_garbage = [
#         r'(\n\*\*\*\*[ï¼š:]\s*)+$',
#         r'(\n\*\*[ï¼š:]\s*)+$',
#         r'(\n\s*[â€»\-\*]+\s*)+$',
#     ]
#     for pattern in trailing_garbage:
#         text = re.sub(pattern, '', text)
    
#     # 7. ç§»é™¤è¿‡å¤šçš„emojiï¼ˆä¿ç•™å‰3ä¸ªï¼Œåˆ é™¤åé¢çš„ï¼‰
#     # æ³¨æ„ï¼šå¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰emojiï¼Œè¿™ä¸ªé€»è¾‘ä¸åº”è¯¥å½±å“æ–‡æœ¬
#     emoji_pattern = r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿â“‚-ğŸ‰‘]+'
#     emoji_matches = list(re.finditer(emoji_pattern, text))
#     if len(emoji_matches) > 3:
#         # ä¿ç•™å‰3ä¸ªemojiï¼Œåˆ é™¤åé¢çš„
#         keep_end = emoji_matches[2].end() if len(emoji_matches) > 2 else len(text)
#         # æ‰¾åˆ°keep_endä¹‹åç¬¬ä¸€ä¸ªéemojiå­—ç¬¦çš„ä½ç½®
#         remaining_text = text[keep_end:]
#         # ç§»é™¤æ‰€æœ‰emoji
#         remaining_text = re.sub(emoji_pattern, '', remaining_text)
#         text = text[:keep_end] + remaining_text
#     # å¦‚æœæ²¡æœ‰è¶…è¿‡3ä¸ªemojiï¼Œä¸åšä»»ä½•å¤„ç†
    
#     # 8. ç§»é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·ï¼ˆè¶…è¿‡2ä¸ªè¿ç»­çš„æ ‡ç‚¹ï¼‰
#     text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š])\1{2,}', r'\1', text)
#     text = re.sub(r'([.!?,;:])\1{2,}', r'\1', text)
    
#     # 8.5. åœ¨å¥å­ç»“å°¾å¤„æˆªæ–­ï¼ˆå¦‚æœåé¢è·Ÿç€åƒåœ¾å†…å®¹ï¼‰
#     sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï½', 'ã€œ']
#     for ending in sentence_endings:
#         last_pos = text.rfind(ending)
#         if last_pos > 0:
#             after = text[last_pos + 1:].strip()
#             # å¦‚æœåé¢è·Ÿç€åƒåœ¾æ¨¡å¼ï¼Œæˆªæ–­
#             if after and (
#                 after.startswith(('ï¼ˆ', '(', 'â€»', '****', '**', 'å•é¡Œ', '---', 'stage')) or
#                 re.match(r'^[A-Z]\)', after)
#             ):
#                 text = text[:last_pos + 1].strip()
#                 break
    
#     # 9. æŒ‰å¥å­è¾¹ç•Œæˆªæ–­ï¼ˆå¦‚æœå¤ªé•¿ï¼‰
#     text = text.strip()
#     if len(text) > max_length:
#         # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
#         sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?])', text)
#         truncated = ""
#         for i in range(0, len(sentences), 2):
#             if i + 1 < len(sentences):
#                 candidate = truncated + sentences[i] + sentences[i+1]
#             else:
#                 candidate = truncated + sentences[i]
            
#             if len(candidate) <= max_length:
#                 truncated = candidate
#             else:
#                 break
        
#         if truncated and len(truncated) > 0:
#             text = truncated
#         else:
#             # å¦‚æœæ‰¾ä¸åˆ°å¥å­è¾¹ç•Œï¼Œç›´æ¥æˆªæ–­
#             text = text[:max_length].rstrip()
#             # å°è¯•åœ¨æœ€åä¸€ä¸ªæ ‡ç‚¹å¤„æˆªæ–­
#             last_punct = max(
#                 text.rfind('ã€‚'), text.rfind('ï¼'), text.rfind('ï¼Ÿ'),
#                 text.rfind('.'), text.rfind('!'), text.rfind('?'),
#                 text.rfind('ï¼Œ'), text.rfind(',')
#             )
#             if last_punct > max_length * 0.5:  # è‡³å°‘ä¿ç•™ä¸€åŠé•¿åº¦
#                 text = text[:last_punct + 1]
#             # å¦‚æœæˆªæ–­åä¸ºç©ºï¼Œè‡³å°‘ä¿ç•™å‰max_lengthä¸ªå­—ç¬¦
#             if not text.strip():
#                 text = text[:max_length].strip()
    
#     # 10. å¤„ç†å¤šè¯­è¨€æ±¡æŸ“ï¼ˆå¦‚æœç›®æ ‡æ˜¯æ—¥è¯­ï¼Œè¿‡æ»¤æ‰å¤§æ®µçš„ä¸­æ–‡è§£é‡Šï¼‰
#     # é’ˆå¯¹"ä¸­æ–‡æ¨ç†+æ—¥è¯­å›ç­”"çš„æƒ…å†µï¼Œè¿›è¡Œå¼ºåŠ›æˆªæ–­
#     # æ£€æµ‹æ˜¯å¦åŒ…å«å¤§é‡ä¸­æ–‡å­—ç¬¦å’Œå°‘é‡æ—¥æ–‡å­—ç¬¦ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡æ¨ç†+æ—¥è¯­å›ç­”ï¼‰
#     chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
#     japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]', text))
#     total_cjk_chars = chinese_chars + japanese_chars
    
#     if total_cjk_chars > 10:
#         chinese_ratio = chinese_chars / total_cjk_chars if total_cjk_chars > 0 else 0
#         japanese_ratio = japanese_chars / total_cjk_chars if total_cjk_chars > 0 else 0
        
#         # å¦‚æœä¸­æ–‡æ¯”ä¾‹å¾ˆé«˜ï¼ˆ>70%ï¼‰ï¼Œå¯èƒ½æ˜¯ä¸­æ–‡æ¨ç†ï¼Œå°è¯•æ‰¾åˆ°æ—¥è¯­å›ç­”çš„å¼€å§‹ä½ç½®
#         if chinese_ratio > 0.7 and japanese_ratio > 0.1:
#             # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿ç»­çš„æ—¥è¯­æ®µè½ï¼ˆè‡³å°‘10ä¸ªå­—ç¬¦ï¼‰
#             japanese_pattern = r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]{10,}'
#             japanese_matches = list(re.finditer(japanese_pattern, text))
            
#             if japanese_matches:
#                 # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ—¥è¯­æ®µè½ï¼Œä¿ç•™ä»è¯¥ä½ç½®å¼€å§‹çš„å†…å®¹
#                 first_japanese_start = japanese_matches[0].start()
#                 # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰æ˜æ˜¾çš„åˆ†éš”ç¬¦ï¼ˆå¦‚"å›ç­”ï¼š"ã€"è¿”ç­”ï¼š"ç­‰ï¼‰
#                 before_japanese = text[:first_japanese_start]
#                 separators = ['å›ç­”ï¼š', 'è¿”ç­”ï¼š', 'å¿œç­”ï¼š', 'ç­”ï¼š', 'ï¼š', '\n\n', 'ã€‚\n']
                
#                 # å¯»æ‰¾æœ€åä¸€ä¸ªåˆ†éš”ç¬¦çš„ä½ç½®
#                 last_sep_pos = -1
#                 for sep in separators:
#                     pos = before_japanese.rfind(sep)
#                     if pos > last_sep_pos:
#                         last_sep_pos = pos + len(sep)
                
#                 if last_sep_pos > 0:
#                     # ä»åˆ†éš”ç¬¦åå¼€å§‹ä¿ç•™
#                     text = text[last_sep_pos:].strip()
#                 else:
#                     # ä»ç¬¬ä¸€ä¸ªæ—¥è¯­æ®µè½å¼€å§‹ä¿ç•™
#                     text = text[first_japanese_start:].strip()
        
#         # å¦‚æœæ–‡æœ¬å¼€å¤´æ˜¯ä¸­æ–‡æ¨ç†æ¨¡å¼ï¼ˆåŒ…å«"åˆ†æ"ã€"å»ºè®®"ã€"éœ€è¦"ç­‰ï¼‰ï¼Œå°è¯•æˆªæ–­
#         if text.startswith(('åˆ†æ', 'å»ºè®®', 'éœ€è¦', 'æ ¹æ®', 'åŸºäº', 'æˆ‘è®¤ä¸º', 'æˆ‘è§‰å¾—')):
#             # å¯»æ‰¾ç¬¬ä¸€ä¸ªæ—¥è¯­æ®µè½æˆ–æ˜æ˜¾çš„å›ç­”æ ‡è®°
#             answer_markers = ['å›ç­”ï¼š', 'è¿”ç­”ï¼š', 'å¿œç­”ï¼š', 'ç­”ï¼š']
#             for marker in answer_markers:
#                 if marker in text:
#                     text = text.split(marker, 1)[-1].strip()
#                     break
            
#             # å¦‚æœæ²¡æ‰¾åˆ°æ ‡è®°ï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ªè¿ç»­çš„æ—¥è¯­æ®µè½
#             if any(marker in text for marker in answer_markers) == False:
#                 japanese_matches = list(re.finditer(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9faf]{5,}', text))
#                 if japanese_matches:
#                     text = text[japanese_matches[0].start():].strip()
    
#     # æœ€ç»ˆæ¸…ç†
#     text = text.strip()
    
#     # å¦‚æœæ¸…ç†åä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå°è¯•ä¿ç•™åŸå§‹æ–‡æœ¬çš„ä¸€éƒ¨åˆ†
#     if not text or len(text) < 5:
#         # å¦‚æœåŸå§‹æ–‡æœ¬å­˜åœ¨ï¼Œè‡³å°‘ä¿ç•™å‰max_lengthä¸ªå­—ç¬¦
#         if original_text and len(original_text.strip()) > 0:
#             # åªåšæœ€åŸºæœ¬çš„æ¸…ç†ï¼šç§»é™¤æ˜æ˜¾çš„åƒåœ¾token
#             fallback_text = original_text
#             for pattern in [r'\bVRTX\b', r'\bVERTEX\b', r'<Vertex>', r'\(Vertex\)', r'_VERTEX_',
#                            r'\bPorno\b', r'\bporno\b', r'Viagra\s+Porno']:
#                 fallback_text = re.sub(pattern, '', fallback_text, flags=re.IGNORECASE)
#             fallback_text = fallback_text.strip()
#             if fallback_text:
#                 text = fallback_text[:max_length] if len(fallback_text) > max_length else fallback_text
    
#     return text.strip()



def process_scenario(
    scenario_path: str,
    checkpoint_dir: str,
    config_name: str,
    use_profile: bool,
    use_history: bool,
    use_context: bool = True,
    num_samples: int = 5,
    output_dir: str = None,
    gpu_id: int = 2,
    log_file_path: str = None,
    base_model_path: str = None,
    max_new_tokens: int = 512,  # é»˜è®¤å¢åŠ åˆ°512
    max_output_length: int = 200,
    use_multi_gpu: bool = False,  # æ˜¯å¦ä½¿ç”¨å¤š GPU åˆ†å¸ƒå¼åŠ è½½
    use_deepspeed: bool = False  # æ˜¯å¦ä½¿ç”¨ DeepSpeed ZeRO-3
):
    """
    å¤„ç†å•ä¸ªåœºæ™¯ï¼šç”Ÿæˆtest_leaderboard.json
    
    Args:
        scenario_path: åœºæ™¯ç›®å½•è·¯å¾„
        checkpoint_dir: æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•
        config_name: é…ç½®åç§°ï¼ˆç”¨äºè¾“å‡ºç›®å½•å‘½åï¼‰
        use_profile: æ˜¯å¦ä½¿ç”¨profile
        use_history: æ˜¯å¦ä½¿ç”¨history
        use_context: æ˜¯å¦ä½¿ç”¨context
        num_samples: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„continuationæ•°é‡
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™åœ¨scenario_pathä¸‹åˆ›å»ºï¼‰
    """
    print(f"\nå¤„ç†åœºæ™¯: {scenario_path}")
    print(f"æ¨¡å‹: {checkpoint_dir}")
    print(f"é…ç½®: profile={use_profile}, history={use_history}, context={use_context}")
    base_model_path = "/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507"
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_leaderboard_path = os.path.join(scenario_path, 'test_leaderboard.json')
    if not os.path.exists(test_leaderboard_path):
        print(f"è­¦å‘Š: {test_leaderboard_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return
    
    test_data = load_test_leaderboard(test_leaderboard_path)
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
    
    # åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆç”¨äºè·å–ç”¨æˆ·ä¿¡æ¯å’Œå†å²ï¼‰
    train_path = os.path.join(scenario_path, 'train.json')
    train_data = load_train_data(train_path) if os.path.exists(train_path) else []
    all_train_samples = extract_training_samples(train_data) if train_data else []
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(all_train_samples)}")
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ç±»å‹
    print(f"æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {base_model_path}")
    model_files = check_model_files(base_model_path)
    print(f"  å‘ç° safetensors æ–‡ä»¶: {len(model_files['safetensors_files'])} ä¸ª")
    print(f"  å‘ç° PyTorch æ–‡ä»¶: {len(model_files['pytorch_files'])} ä¸ª")
    
    # å°è¯•åŠ è½½ tokenizerï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    tokenizer_json_path = os.path.join(checkpoint_dir, 'tokenizer.json')

    print("åŠ è½½ tokenizerï¼ˆå¿…é¡»ä¸æ¨¡å‹ checkpoint å®Œå…¨ä¸€è‡´ï¼‰")

    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        # checkpoint_dir,
        use_fast=False,          # å¼ºçƒˆå»ºè®®
        trust_remote_code=True,
        local_files_only=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("âœ“ Tokenizer åŠ è½½æˆåŠŸ")
    print(f"  vocab_size: {tokenizer.vocab_size}")
    print(f"  pad_token_id: {tokenizer.pad_token_id}")
    print(f"  eos_token_id: {tokenizer.eos_token_id}")




    
    # éªŒè¯ tokenizer é…ç½®
    vocab_size = len(tokenizer)
    print(f"  Tokenizer è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  pad_token_id: {tokenizer.pad_token_id}")
    print(f"  eos_token_id: {tokenizer.eos_token_id}")
    print(f"  bos_token_id: {tokenizer.bos_token_id}")
    
    # åŠ è½½æ¨¡å‹
    print("  åŠ è½½æ¨¡å‹æƒé‡...")
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨å¤š GPU æˆ– DeepSpeed æ¥å†³å®šè®¾å¤‡è®¾ç½®
    # DeepSpeed ä¼šè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰ GPUï¼Œæ‰€ä»¥ä¹Ÿåº”è¯¥è¢«è§†ä¸ºå¤š GPU æ¨¡å¼
    # æ£€æŸ¥ DeepSpeed æ˜¯å¦å¯ç”¨ï¼ˆåœ¨å‡½æ•°å†…éƒ¨æ£€æŸ¥ï¼Œç¡®ä¿æ­£ç¡®ï¼‰
    deepspeed_available = False
    if use_deepspeed:
        try:
            import deepspeed
            deepspeed_available = True
        except ImportError:
            deepspeed_available = False
            print("  è­¦å‘Š: DeepSpeed æœªå®‰è£…ï¼Œå°†å›é€€åˆ°æ™®é€šåŠ è½½æ–¹å¼")
    
    if torch.cuda.is_available():
        if use_multi_gpu or (use_deepspeed and deepspeed_available):
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ GPU
            num_gpus = torch.cuda.device_count()
            if use_deepspeed and deepspeed_available:
                print(f"  ä½¿ç”¨ DeepSpeed å¤š GPU æ¨¡å¼ï¼Œå¯ç”¨ GPU æ•°é‡: {num_gpus}")
            else:
                print(f"  ä½¿ç”¨å¤š GPU æ¨¡å¼ï¼Œå¯ç”¨ GPU æ•°é‡: {num_gpus}")
            print(f"  å°†ä½¿ç”¨ GPU: {list(range(num_gpus))}")
            target_device = None  # device_map="auto" æˆ– DeepSpeed ä¼šè‡ªåŠ¨åˆ†é…
        else:
            # ä½¿ç”¨æŒ‡å®šçš„å•ä¸ª GPU
            target_device = torch.device(f'cuda:{gpu_id}')
            print(f"  ä½¿ç”¨å• GPU æ¨¡å¼ï¼Œç›®æ ‡è®¾å¤‡: {target_device}")
    else:
        target_device = torch.device('cpu')
        print(f"  ç›®æ ‡è®¾å¤‡: {target_device}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA æ¨¡å‹ï¼ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨ adapter_config.jsonï¼‰
    adapter_config_path = os.path.join(checkpoint_dir, 'adapter_config.json')
    is_lora_model = os.path.exists(adapter_config_path)
    
    if is_lora_model and PEFT_AVAILABLE:
        print("  æ£€æµ‹åˆ° LoRA æ¨¡å‹ï¼Œä½¿ç”¨ PeftModel åŠ è½½...")
        # é¦–å…ˆéœ€è¦åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œç„¶ååŠ è½½ LoRA é€‚é…å™¨
        # ä» adapter_config.json è¯»å–åŸºç¡€æ¨¡å‹è·¯å¾„
        try:
            with open(adapter_config_path, 'r', encoding='utf-8') as f:
                adapter_config = json.load(f)
            base_model_name_or_path = adapter_config.get('base_model_name_or_path', None)
            
            # å¦‚æœé…ç½®ä¸­æ²¡æœ‰åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œå°è¯•ä»å‚æ•°æˆ–å¸¸è§è·¯å¾„æ¨æ–­
            if base_model_name_or_path is None or not os.path.exists(base_model_name_or_path):
                # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°
                if base_model_path and os.path.exists(base_model_path):
                    base_model_name_or_path = base_model_path
                else:
                    # å°è¯•å¸¸è§çš„æ¨¡å‹è·¯å¾„
                    possible_paths = [
                        '/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507',
                        '/mnt/parallel/models/Qwen3-8B',
                        '/mnt/parallel/models/Qwen3-4B',
                    ]
                    for path in possible_paths:
                        if os.path.exists(path):
                            base_model_name_or_path = path
                            break
                
                if base_model_name_or_path is None or not os.path.exists(base_model_name_or_path):
                    raise RuntimeError(f"æ— æ³•æ‰¾åˆ° LoRA æ¨¡å‹çš„åŸºç¡€æ¨¡å‹è·¯å¾„ã€‚è¯·ä½¿ç”¨ --base_model_path å‚æ•°æŒ‡å®šã€‚")
            
            print(f"  åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_name_or_path}")
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            # é€‰æ‹©æ•°æ®ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨ bfloat16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ float16
            if torch.cuda.is_available():
                if torch.cuda.is_bf16_supported():
                    lora_dtype = torch.bfloat16
                else:
                    lora_dtype = torch.float16
            else:
                lora_dtype = torch.float32
            
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name_or_path,
                torch_dtype=lora_dtype,
                device_map=None,
                trust_remote_code=True,
                local_files_only=True
            )
            
            # åŠ è½½ LoRA é€‚é…å™¨
            # æ³¨æ„ï¼šéœ€è¦æŒ‡å®š local_files_only=True ä»¥é¿å…å°†æœ¬åœ°è·¯å¾„è¯¯è®¤ä¸º HuggingFace Hub repo id
            model = PeftModel.from_pretrained(
                base_model,
                checkpoint_dir,
                torch_dtype=lora_dtype,
                local_files_only=True
            )
            
            # ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
            model = model.to(target_device)
            print("  âœ“ LoRA æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"  åŠ è½½ LoRA æ¨¡å‹å¤±è´¥: {e}")
            print("  å°è¯•ä½œä¸ºæ™®é€šæ¨¡å‹åŠ è½½...")
            is_lora_model = False
    
    if not is_lora_model:
        # åŠ è½½æ™®é€šæ¨¡å‹ï¼ˆé LoRAï¼‰
        try:
            if use_deepspeed and deepspeed_available and torch.cuda.is_available():
                # DeepSpeed çš„ tensor_parallel éœ€è¦ MPI æˆ–åˆ†å¸ƒå¼ç¯å¢ƒ
                # å¯¹äºå•æœºå¤š GPUï¼Œæˆ‘ä»¬ä½¿ç”¨ device_map="auto" æ›´ç®€å•å¯é 
                print("  æ³¨æ„: DeepSpeed tensor_parallel éœ€è¦ MPI/åˆ†å¸ƒå¼ç¯å¢ƒ")
                print("  å›é€€åˆ°ä½¿ç”¨ device_map='auto' æ–¹å¼åŠ è½½ï¼ˆæ›´ç®€å•å¯é ï¼‰...")
                # é€‰æ‹©æ•°æ®ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨ bfloat16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ float16
                if torch.cuda.is_bf16_supported():
                    dtype = torch.bfloat16
                    dtype_str = "bfloat16"
                else:
                    dtype = torch.float16
                    dtype_str = "float16"
                print(f"  ä½¿ç”¨æ•°æ®ç±»å‹: {dtype_str}")
                
                # ä½¿ç”¨ device_map="auto" é…åˆ max_memory å’Œ low_cpu_mem_usage
                # å¦‚æœ safetensors header å¤ªå¤§ï¼Œå°è¯•ä½¿ç”¨ PyTorch æ ¼å¼
                max_memory = {i: "20GiB" for i in range(torch.cuda.device_count())}
                
                # æ ¹æ®å¯ç”¨æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½ç­–ç•¥
                load_success = False
                last_error = None
                
                    # ç­–ç•¥1: å¦‚æœæœ‰ PyTorch æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ï¼ˆtransformers ä¼šè‡ªåŠ¨é€‰æ‹© PyTorch æ–‡ä»¶ï¼‰
                    if model_files['has_pytorch']:
                        print("  æ£€æµ‹åˆ° PyTorch æ ¼å¼æ–‡ä»¶ï¼Œtransformers å°†è‡ªåŠ¨ä½¿ç”¨ PyTorch æ ¼å¼...")
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=dtype,
                                device_map="auto",
                                max_memory=max_memory,
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ PyTorch æ ¼å¼åŠ è½½æˆåŠŸ")
                            if hasattr(model, 'hf_device_map'):
                                print(f"  æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {model.hf_device_map}")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  PyTorch æ ¼å¼åŠ è½½å¤±è´¥: {e}")
                
                # ç­–ç•¥2: å¦‚æœ PyTorch åŠ è½½å¤±è´¥æˆ–åªæœ‰ safetensorsï¼Œå°è¯• safetensors
                if not load_success and model_files['has_safetensors']:
                    print("  å°è¯•ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½...")
                    try:
                        model = AutoModelForCausalLM.from_pretrained(
                            base_model_path,
                            torch_dtype=dtype,
                            device_map="auto",
                            max_memory=max_memory,
                            low_cpu_mem_usage=True,
                            trust_remote_code=True,
                            local_files_only=True
                        )
                        print("  âœ“ ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½æˆåŠŸ")
                        if hasattr(model, 'hf_device_map'):
                            print(f"  æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {model.hf_device_map}")
                        load_success = True
                    except Exception as e:
                        last_error = e
                        if "header too large" in str(e) or "SafetensorError" in str(e):
                            print(f"  safetensors header å¤ªå¤§: {e}")
                        else:
                            print(f"  safetensors åŠ è½½å¤±è´¥: {e}")
                
                # ç­–ç•¥3: ä½¿ç”¨ accelerate åº“çš„åˆ†ç‰‡åŠ è½½ï¼ˆé€‚ç”¨äº header too large é—®é¢˜ï¼‰
                if not load_success:
                    print("  å°è¯•ä½¿ç”¨ accelerate åº“åˆ†ç‰‡åŠ è½½...")
                    try:
                        from accelerate import load_checkpoint_and_dispatch, init_empty_weights
                        from transformers import AutoConfig
                        
                        print("  ä½¿ç”¨ accelerate åº“åˆ†ç‰‡åŠ è½½...")
                        config = AutoConfig.from_pretrained(
                            base_model_path,
                            trust_remote_code=True,
                            local_files_only=True
                        )
                        
                        # å…ˆåˆ›å»ºç©ºæ¨¡å‹
                        with init_empty_weights():
                            model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
                        
                        # ä½¿ç”¨ accelerate åˆ†ç‰‡åŠ è½½æƒé‡
                        # accelerate ä¼šè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æ ¼å¼ï¼ˆä¼˜å…ˆ PyTorchï¼‰
                        model = load_checkpoint_and_dispatch(
                            model,
                            base_model_path,
                            device_map="auto",
                            max_memory=max_memory,
                            dtype=dtype,
                            no_split_module_classes=[]  # è®© accelerate è‡ªåŠ¨å†³å®šå¦‚ä½•åˆ†ç‰‡
                        )
                        print("  âœ“ ä½¿ç”¨ accelerate åº“åŠ è½½æˆåŠŸ")
                        load_success = True
                    except Exception as e2:
                        last_error = e2
                        print(f"  accelerate åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                
                # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºè¯¦ç»†é”™è¯¯
                if not load_success:
                    error_msg = f"""
æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}

æ¨¡å‹æ–‡ä»¶æ£€æŸ¥ç»“æœ:
  - Safetensors æ–‡ä»¶: {len(model_files['safetensors_files'])} ä¸ª
  - PyTorch æ–‡ä»¶: {len(model_files['pytorch_files'])} ä¸ª

å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š
1. Safetensors header å¤ªå¤§ï¼šè¿™æ˜¯ Qwen3-30B-A3B æ¨¡å‹çš„å·²çŸ¥é—®é¢˜
   - è§£å†³æ–¹æ¡ˆ A: å°† safetensors è½¬æ¢ä¸º PyTorch æ ¼å¼
     è¿è¡Œ: python -c "from transformers import AutoModel; model = AutoModel.from_pretrained('{base_model_path}', trust_remote_code=True); model.save_pretrained('{base_model_path}_pytorch', safe_serialization=False)"
     ç„¶åä½¿ç”¨è½¬æ¢åçš„è·¯å¾„: {base_model_path}_pytorch
   - è§£å†³æ–¹æ¡ˆ B: æ›´æ–° safetensors åº“åˆ°æœ€æ–°ç‰ˆæœ¬
     pip install --upgrade safetensors transformers accelerate
   - è§£å†³æ–¹æ¡ˆ C: ä½¿ç”¨æ›´å°‘çš„ GPUï¼ˆå‡å°‘åˆ° 4 å¼ æˆ–æ›´å°‘ï¼‰
   - è§£å†³æ–¹æ¡ˆ D: ä½¿ç”¨å• GPU æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ --use_multi_gpuï¼‰

2. å†…å­˜ä¸è¶³ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ç³»ç»Ÿå†…å­˜ï¼ˆå»ºè®®è‡³å°‘ 64GBï¼‰

3. æ¨¡å‹æ–‡ä»¶æŸåï¼šæ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§

å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æ¨¡å‹æä¾›è€…è·å– PyTorch æ ¼å¼çš„æƒé‡æ–‡ä»¶ã€‚
"""
                    raise RuntimeError(error_msg)
            elif torch.cuda.is_available():
                # é€‰æ‹©æ•°æ®ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨ bfloat16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ float16
                if torch.cuda.is_bf16_supported():
                    dtype = torch.bfloat16
                    dtype_str = "bfloat16"
                else:
                    dtype = torch.float16
                    dtype_str = "float16"
                print(f"  ä½¿ç”¨æ•°æ®ç±»å‹: {dtype_str}")
                
                if use_multi_gpu:
                    # å¤š GPU æ¨¡å¼ï¼šä½¿ç”¨ device_map="auto" è‡ªåŠ¨åœ¨æ‰€æœ‰ GPU ä¸Šåˆ†å¸ƒæ¨¡å‹
                    print("  ä½¿ç”¨å¤š GPU åˆ†å¸ƒå¼åŠ è½½...")
                    # è®¡ç®—æ¯ä¸ª GPU çš„æœ€å¤§å†…å­˜ï¼ˆå¯é€‰ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨åˆ†é…ï¼‰
                    max_memory = {i: "20GiB" for i in range(torch.cuda.device_count())}
                    
                    # æ ¹æ®å¯ç”¨æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½ç­–ç•¥
                    load_success = False
                    last_error = None
                    
                    # ç­–ç•¥1: å¦‚æœæœ‰ PyTorch æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ï¼ˆtransformers ä¼šè‡ªåŠ¨é€‰æ‹© PyTorch æ–‡ä»¶ï¼‰
                    if model_files['has_pytorch']:
                        print("  æ£€æµ‹åˆ° PyTorch æ ¼å¼æ–‡ä»¶ï¼Œtransformers å°†è‡ªåŠ¨ä½¿ç”¨ PyTorch æ ¼å¼...")
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=dtype,
                                device_map="auto",
                                max_memory=max_memory,
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ PyTorch æ ¼å¼å¤š GPU åŠ è½½æˆåŠŸ")
                            if hasattr(model, 'hf_device_map'):
                                print(f"  æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {model.hf_device_map}")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  PyTorch æ ¼å¼åŠ è½½å¤±è´¥: {e}")
                    
                    # ç­–ç•¥2: å¦‚æœ PyTorch åŠ è½½å¤±è´¥æˆ–åªæœ‰ safetensorsï¼Œå°è¯• safetensors
                    if not load_success and model_files['has_safetensors']:
                        print("  å°è¯•ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½...")
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=dtype,
                                device_map="auto",
                                max_memory=max_memory,
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ safetensors æ ¼å¼å¤š GPU åŠ è½½æˆåŠŸ")
                            if hasattr(model, 'hf_device_map'):
                                print(f"  æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {model.hf_device_map}")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            if "header too large" in str(e) or "SafetensorError" in str(e):
                                print(f"  safetensors header å¤ªå¤§: {e}")
                            else:
                                print(f"  safetensors åŠ è½½å¤±è´¥: {e}")
                    
                    # å¦‚æœéƒ½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
                    if not load_success:
                        raise RuntimeError(f"å¤š GPU åŠ è½½å¤±è´¥: {last_error}\nè¯·å°è¯•ä½¿ç”¨å• GPU æ¨¡å¼æˆ–è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼")
                else:
                    # å• GPU æ¨¡å¼ï¼šå¯¹äº MoE æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨ device_map="auto" è‡ªåŠ¨åˆ†é…è®¾å¤‡
                    # è¿™æ ·å¯ä»¥æ›´å¥½åœ°å¤„ç†å¤§æ¨¡å‹å’Œ MoE æ¶æ„
                    load_success = False
                    last_error = None
                    
                    # ç­–ç•¥1: å¦‚æœæœ‰ PyTorch æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ï¼ˆtransformers ä¼šè‡ªåŠ¨é€‰æ‹© PyTorch æ–‡ä»¶ï¼‰
                    if model_files['has_pytorch']:
                        print("  æ£€æµ‹åˆ° PyTorch æ ¼å¼æ–‡ä»¶ï¼Œtransformers å°†è‡ªåŠ¨ä½¿ç”¨ PyTorch æ ¼å¼...")
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=dtype,
                                device_map="auto",
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ PyTorch æ ¼å¼åŠ è½½æˆåŠŸ")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  PyTorch æ ¼å¼åŠ è½½å¤±è´¥: {e}")
                    
                    # ç­–ç•¥2: å¦‚æœ PyTorch åŠ è½½å¤±è´¥æˆ–åªæœ‰ safetensorsï¼Œå°è¯• safetensors
                    if not load_success and model_files['has_safetensors']:
                        print("  å°è¯•ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½...")
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=dtype,
                                device_map="auto",
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½æˆåŠŸ")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            if "header too large" in str(e) or "SafetensorError" in str(e):
                                print(f"  safetensors header å¤ªå¤§: {e}")
                            else:
                                print(f"  safetensors åŠ è½½å¤±è´¥: {e}")
                    
                    # ç­–ç•¥3: å›é€€åˆ°æŒ‡å®šå•ä¸ªè®¾å¤‡
                    if not load_success:
                        print(f"  å›é€€åˆ°æŒ‡å®šè®¾å¤‡åŠ è½½: {target_device}")
                        try:
                            # transformers ä¼šè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æ ¼å¼ï¼ˆä¼˜å…ˆ PyTorchï¼‰
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=dtype,
                                device_map={"": target_device},
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨æŒ‡å®šè®¾å¤‡åŠ è½½æˆåŠŸ")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  æŒ‡å®šè®¾å¤‡åŠ è½½ä¹Ÿå¤±è´¥: {e}")
                    
                    # å¦‚æœéƒ½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
                    if not load_success:
                        raise RuntimeError(f"å• GPU åŠ è½½å¤±è´¥: {last_error}\nè¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼")
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_path,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    local_files_only=True
                )
                model = model.to(target_device)
        except Exception as e:
            print(f"  ä½¿ç”¨ device_map åŠ è½½å¤±è´¥: {e}")
            print("  å°è¯•ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼åŠ è½½...")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼ï¼ˆMoE æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
            # é€‰æ‹©æ•°æ®ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨ bfloat16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ float16
            if torch.cuda.is_available():
                if torch.cuda.is_bf16_supported():
                    fallback_dtype = torch.bfloat16
                else:
                    fallback_dtype = torch.float16
            else:
                fallback_dtype = torch.float32
                
            try:
                load_success = False
                last_error = None
                
                if use_multi_gpu and torch.cuda.is_available():
                    # å¤š GPU å›é€€ï¼šä½¿ç”¨ max_memory é™åˆ¶å’Œ low_cpu_mem_usage
                    max_memory = {i: "20GiB" for i in range(torch.cuda.device_count())}
                    
                    # ä¼˜å…ˆä½¿ç”¨ PyTorch æ ¼å¼ï¼ˆtransformers ä¼šè‡ªåŠ¨é€‰æ‹©ï¼‰
                    if model_files['has_pytorch']:
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=fallback_dtype,
                                device_map="auto",
                                max_memory=max_memory,
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ PyTorch æ ¼å¼å¤š GPU å›é€€æ–¹å¼åŠ è½½æˆåŠŸ")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  PyTorch æ ¼å¼å¤š GPU å›é€€å¤±è´¥: {e}")
                    
                    # å¦‚æœ PyTorch å¤±è´¥ï¼Œå°è¯• safetensors
                    if not load_success and model_files['has_safetensors']:
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=fallback_dtype,
                                device_map="auto",
                                max_memory=max_memory,
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ safetensors æ ¼å¼å¤š GPU å›é€€æ–¹å¼åŠ è½½æˆåŠŸ")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  safetensors æ ¼å¼å¤š GPU å›é€€å¤±è´¥: {e}")
                else:
                    # å• GPU å›é€€ï¼šå°è¯•ä½¿ç”¨ device_map="auto" è‡ªåŠ¨åˆ†é…è®¾å¤‡
                    if model_files['has_pytorch']:
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=fallback_dtype,
                                device_map="auto",
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ PyTorch æ ¼å¼ device_map='auto' åŠ è½½æˆåŠŸ")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  PyTorch æ ¼å¼ device_map='auto' å¤±è´¥: {e}")
                    
                    # å¦‚æœ PyTorch å¤±è´¥ï¼Œå°è¯• safetensors
                    if not load_success and model_files['has_safetensors']:
                        try:
                            model = AutoModelForCausalLM.from_pretrained(
                                base_model_path,
                                torch_dtype=fallback_dtype,
                                device_map="auto",
                                low_cpu_mem_usage=True,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            print("  âœ“ ä½¿ç”¨ safetensors æ ¼å¼ device_map='auto' åŠ è½½æˆåŠŸ")
                            load_success = True
                        except Exception as e:
                            last_error = e
                            print(f"  safetensors æ ¼å¼ device_map='auto' å¤±è´¥: {e}")
                
                # å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•æœ€ç®€å•çš„åŠ è½½æ–¹å¼
                if not load_success:
                    print("  å°è¯•æœ€ç®€å•çš„åŠ è½½æ–¹å¼...")
                    # æœ€ç®€å•çš„åŠ è½½æ–¹å¼ï¼Œä¸æŒ‡å®šè®¾å¤‡æ˜ å°„
                    # transformers ä¼šè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æ ¼å¼ï¼ˆä¼˜å…ˆ PyTorchï¼‰
                    try:
                        model = AutoModelForCausalLM.from_pretrained(
                            base_model_path,
                            torch_dtype=fallback_dtype,
                            trust_remote_code=True,
                            local_files_only=True
                        )
                        # æ‰‹åŠ¨ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆå¦‚æœæŒ‡å®šäº†å• GPUï¼‰
                        if target_device is not None:
                            model = model.to(target_device)
                        print("  âœ“ ä½¿ç”¨ç®€å•æ–¹å¼åŠ è½½æˆåŠŸ")
                        load_success = True
                    except Exception as e:
                        last_error = e
                        print(f"  ç®€å•æ–¹å¼åŠ è½½ä¹Ÿå¤±è´¥: {e}")
                        raise RuntimeError(f"æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}\nè¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼")
            except Exception as e2:
                # å¦‚æœä¸Šé¢çš„æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºè¯¦ç»†é”™è¯¯
                error_msg = f"""
æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ã€‚é”™è¯¯: {e2}

æ¨¡å‹æ–‡ä»¶æ£€æŸ¥ç»“æœ:
  - Safetensors æ–‡ä»¶: {len(model_files['safetensors_files'])} ä¸ª
  - PyTorch æ–‡ä»¶: {len(model_files['pytorch_files'])} ä¸ª

å»ºè®®çš„è§£å†³æ–¹æ¡ˆï¼š
1. å°† safetensors è½¬æ¢ä¸º PyTorch æ ¼å¼:
   python -c "from transformers import AutoModel; model = AutoModel.from_pretrained('{base_model_path}', trust_remote_code=True); model.save_pretrained('{base_model_path}_pytorch', safe_serialization=False)"

2. æ›´æ–°åº“ç‰ˆæœ¬:
   pip install --upgrade safetensors transformers accelerate

3. ä½¿ç”¨å• GPU æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ --use_multi_gpuï¼‰
"""
                raise RuntimeError(error_msg)
            # å…ˆæ£€æŸ¥æ¨¡å‹çŠ¶æ€ï¼Œå†ç§»åŠ¨åˆ°è®¾å¤‡
            try:
                # éªŒè¯æ¨¡å‹æƒé‡æ˜¯å¦æœ‰æ•ˆ
                if hasattr(model, 'lm_head'):
                    weight = model.lm_head.weight
                    if torch.isnan(weight).any() or torch.isinf(weight).any():
                        print("  è­¦å‘Š: æ£€æµ‹åˆ°æ¨¡å‹æƒé‡ä¸­æœ‰ NaN æˆ– Inf å€¼")
            except:
                pass
            
            # å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡
            try:
                model = model.to(target_device)
            except RuntimeError as e:
                if "CUDA" in str(e) or "device-side assert" in str(e):
                    print(f"  CUDA é”™è¯¯: {e}")
                    print("  å°è¯•æ¸…ç† CUDA ç¼“å­˜å¹¶é‡è¯•...")
                    torch.cuda.empty_cache()
                    import time
                    time.sleep(2)
                    model = model.to(target_device)
                else:
                    raise
    
    model.eval()
    print("  æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
    
    # éªŒè¯æ¨¡å‹è¯æ±‡è¡¨å¤§å°
    # model_vocab_size = model.config.vocab_size
    # print(f"  æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {model_vocab_size}")
    # if vocab_size != model_vocab_size:
    #     print(f"  è­¦å‘Š: Tokenizer å’Œæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…ï¼")
    
    # print("æ¨¡å‹åŠ è½½å®Œæˆ")

    model_vocab_size = model.config.vocab_size
    tokenizer_vocab_size = tokenizer.vocab_size

    if model_vocab_size != tokenizer_vocab_size:
        print(f"è­¦å‘Š: tokenizer vocab ({tokenizer_vocab_size}) != model vocab ({model_vocab_size})")
        print("è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸º Qwen3 æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°å¯èƒ½å¤§äº tokenizer çš„è¯æ±‡è¡¨å¤§å°")
        print("æ¨¡å‹ä¼šè‡ªåŠ¨å¤„ç†è¿™ç§æƒ…å†µï¼Œåªè¦ tokenizer çš„è¯æ±‡è¡¨æ˜¯æ¨¡å‹è¯æ±‡è¡¨çš„å­é›†å³å¯")
        
        # éªŒè¯ tokenizer çš„è¯æ±‡è¡¨æ˜¯å¦æ˜¯æ¨¡å‹è¯æ±‡è¡¨çš„å­é›†
        if tokenizer_vocab_size > model_vocab_size:
            raise RuntimeError(
                f"FATAL: tokenizer vocab ({tokenizer_vocab_size}) > model vocab ({model_vocab_size})"
            )
        else:
            print(f"âœ“ Tokenizer è¯æ±‡è¡¨ ({tokenizer_vocab_size}) æ˜¯æ¨¡å‹è¯æ±‡è¡¨ ({model_vocab_size}) çš„å­é›†ï¼Œå¯ä»¥ç»§ç»­")
    else:
        print("âœ“ Tokenizer å’Œæ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°åŒ¹é…")

    
    # è·å–ä»»åŠ¡æè¿°ï¼ˆä»ç¬¬ä¸€ä¸ªæµ‹è¯•æ ·æœ¬ä¸­æå–ï¼Œå¦‚æœæœ‰ï¼‰
    task_description = ""
    if test_data and 'task' in test_data[0]:
        task_description = test_data[0]['task'].get('description', '')
    
    # æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    log_file = None
    is_first_log_entry = True
    if log_file_path:
        log_file_dir = os.path.dirname(log_file_path)
        if log_file_dir:
            os.makedirs(log_file_dir, exist_ok=True)
        log_file = open(log_file_path, 'w', encoding='utf-8')
        log_file.write("[\n")  # å¼€å§‹ JSON æ•°ç»„
        print(f"æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    
    # å¤„ç†æ¯ä¸ªæµ‹è¯•æ ·æœ¬
    print("ç”Ÿæˆ continuations...")
    total_items = 0
    generated_count = 0
    error_count = 0
    skipped_count = 0
    
    for sample_idx, sample in enumerate(tqdm(test_data, desc="ç”Ÿæˆè¿›åº¦")):
        # test_leaderboard.jsonçš„ç»“æ„ï¼šcontextåœ¨task.task_behavior_collections[0].data[0].context
        task = sample.get('task', {})
        collections = task.get('task_behavior_collections', [])
        
        if not collections:
            print(f"è­¦å‘Š: æ ·æœ¬ {sample_idx} ç¼ºå°‘ task_behavior_collectionsï¼Œè·³è¿‡")
            continue
        
        # å¤„ç†æ¯ä¸ªcollectionä¸­çš„data
        for collection in collections:
            data_items = collection.get('data', [])
            for data_item in data_items:
                total_items += 1
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰continuationsï¼ˆéç©ºï¼‰
                existing_conts = data_item.get('continuations', [])
                if existing_conts and len(existing_conts) > 0:
                    skipped_count += 1
                    continue
                
                context = data_item.get('context', [])
                if not context:
                    print(f"è­¦å‘Š: æ ·æœ¬ {sample_idx} ç¼ºå°‘contextï¼Œè·³è¿‡")
                    skipped_count += 1
                    continue
                
                user_info = get_user_info_from_leaderboard(sample, train_data)
                
                # è·å–å†å²è¯æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
                history_evidence = []
                if use_history and user_info['user_train_samples']:
                    history_evidence = user_info['user_train_samples'][-3:]  # ä½¿ç”¨æœ€è¿‘3ä¸ªæ ·æœ¬
                
                # æ„å»ºprompt
                messages = build_prompt(
                    context=context,
                    user_profile=user_info['user_profile'] if use_profile else None,
                    task_description=task_description,
                    history=history_evidence if use_history else None,
                    use_profile=use_profile,
                    use_history=use_history,
                    use_context=use_context
                )
                
                # æ£€æµ‹æ˜¯å¦ä¸ºæ—¥è¯­ä»»åŠ¡ï¼ˆé€šè¿‡åœºæ™¯è·¯å¾„æˆ–ä»»åŠ¡æè¿°åˆ¤æ–­ï¼‰
                is_japanese_task = False
                scenario_name = os.path.basename(scenario_path)
                if 'RealPersonaChat' in scenario_name or 'realpersonachat' in scenario_name.lower():
                    is_japanese_task = True
                elif task_description and ('æ—¥æœ¬èª' in task_description or 'æ—¥è¯­' in task_description or 'Japanese' in task_description):
                    is_japanese_task = True
                # ä¹Ÿå¯ä»¥é€šè¿‡æ£€æŸ¥contextä¸­æ˜¯å¦æœ‰æ—¥è¯­å­—ç¬¦æ¥åˆ¤æ–­
                if not is_japanese_task and context:
                    for turn in context[-3:]:  # æ£€æŸ¥æœ€å3ä¸ªturn
                        content = turn.get('content', '')
                        if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', content):
                            is_japanese_task = True
                            break
                
                # ç”Ÿæˆcontinuations
                # è·å–æ¨¡å‹è®¾å¤‡ï¼ˆå¯¹äºå¤š GPU æ¨¡å‹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨çš„è®¾å¤‡ï¼‰
                # å¦‚æœæ¨¡å‹ä½¿ç”¨ device_mapï¼Œè¾“å…¥ä¼šè‡ªåŠ¨è·¯ç”±åˆ°æ­£ç¡®çš„è®¾å¤‡
                try:
                    model_device = next(model.parameters()).device
                except StopIteration:
                    # å¦‚æœæ¨¡å‹æ²¡æœ‰å‚æ•°ï¼ˆä¸å¤ªå¯èƒ½ï¼‰ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡
                    model_device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
                continuations = []
                
                # ç”Ÿæˆ num_samples ä¸ª continuations
                for _ in range(num_samples):
                    try:
                        result = generate_continuations(
                            model=model,
                            tokenizer=tokenizer,
                            messages=messages,
                            num_samples=1,  # æ¯æ¬¡ç”Ÿæˆ1ä¸ª
                            max_new_tokens=max_new_tokens,  # ä½¿ç”¨å‚æ•°
                            device=model_device,
                            do_sample=True,
                            temperature=0.7,
                            top_p=0.9,
                            repetition_penalty=1.2,  # å¢åŠ é‡å¤æƒ©ç½š
                            no_repeat_ngram_size=4,
                            max_output_length=max_output_length,
                            is_japanese_task=is_japanese_task  # ä¼ é€’æ—¥è¯­ä»»åŠ¡æ ‡è¯†
                        )
                        if result and len(result) > 0:
                            continuations.extend(result)
                    except RuntimeError as e:
                        if "CUDA" in str(e) or "device-side assert" in str(e) or "inf" in str(e) or "nan" in str(e):
                            # å°è¯•ä½¿ç”¨ greedy decoding
                            try:
                                result = generate_continuations(
                                    model=model,
                                    tokenizer=tokenizer,
                                    messages=messages,
                                    num_samples=1,
                                    max_new_tokens=max_new_tokens,  # ä½¿ç”¨å‚æ•°
                                    device=model_device,
                                    do_sample=False,  # Greedy decoding
                                    repetition_penalty=1.2,  # å¢åŠ é‡å¤æƒ©ç½š
                                    no_repeat_ngram_size=4,
                                    max_output_length=max_output_length,
                                    is_japanese_task=is_japanese_task  # ä¼ é€’æ—¥è¯­ä»»åŠ¡æ ‡è¯†
                                )
                                if result and len(result) > 0:
                                    continuations.extend(result)
                            except Exception as e2:
                                print(f"  è­¦å‘Š: ç”Ÿæˆå¤±è´¥: {e2}")
                                break
                        else:
                            raise
                    except Exception as e:
                        print(f"  è­¦å‘Š: ç”Ÿæˆå¤±è´¥: {e}")
                        break
                
                # è·å–å‚è€ƒå€¼ï¼ˆå¦‚æœæœ‰ï¼‰
                reference = data_item.get('continuation', '') or data_item.get('reference', '')
                
                # éªŒè¯ç”Ÿæˆç»“æœ
                if not continuations or len(continuations) == 0:
                    print(f"è­¦å‘Š: æ ·æœ¬ {sample_idx} ç”Ÿæˆäº†ç©ºçš„continuations")
                    error_count += 1
                    error_msg = "ç”Ÿæˆäº†ç©ºçš„continuations"
                else:
                    # å¡«å……åˆ°data_itemä¸­
                    data_item['continuations'] = continuations
                    generated_count += 1
                    if generated_count % 10 == 0:
                        print(f"å·²ç”Ÿæˆ {generated_count} ä¸ªæ ·æœ¬çš„continuations")
                    error_msg = None
                
                # è®°å½•æ—¥å¿—
                if log_file:
                    log_inference_details(
                        log_file=log_file,
                        sample_idx=sample_idx,
                        data_item_idx=total_items - 1,
                        context=context,
                        messages=messages,
                        user_info=user_info,
                        history_evidence=history_evidence,
                        continuations=continuations,
                        reference=reference,
                        error=error_msg,
                        is_first_entry=is_first_log_entry
                    )
                    is_first_log_entry = False
    
    # å…³é—­æ—¥å¿—æ–‡ä»¶
    if log_file:
        log_file.write("\n]")  # å…³é—­ JSON æ•°ç»„
        log_file.close()
        print(f"\nâœ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file_path}")
    
    print(f"\nç”Ÿæˆç»Ÿè®¡:")
    print(f"  æ€»dataé¡¹: {total_items}")
    print(f"  æˆåŠŸç”Ÿæˆ: {generated_count}")
    print(f"  è·³è¿‡: {skipped_count}")
    print(f"  é”™è¯¯: {error_count}")
    
    # ä¿å­˜ç»“æœåˆ° /mnt/parallel/lingyu.li
    if output_dir is None:
        # ä»åœºæ™¯è·¯å¾„æå–æ•°æ®é›†åç§°
        dataset_name = os.path.basename(scenario_path)
        # ä¿å­˜åˆ° /mnt/parallel/lingyu.li/{æ•°æ®é›†å}_{é…ç½®å}/
        base_output_dir = "/mnt/parallel/lingyu.li"
        output_dir = os.path.join(base_output_dir, f"{dataset_name}_{config_name}")
        dataset_name_for_fallback = dataset_name  # ä¿å­˜ç”¨äºé”™è¯¯å¤„ç†
    
    # å°è¯•åˆ›å»ºç›®å½•ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æœ¬åœ°ç›®å½•
    try:
        os.makedirs(output_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
    except (OSError, IOError) as e:
        print(f"è­¦å‘Š: æ— æ³•åœ¨ {output_dir} åˆ›å»ºç›®å½•: {e}")
        # ä½¿ç”¨æœ¬åœ°ç›®å½•ä½œä¸ºå¤‡é€‰
        if 'dataset_name_for_fallback' in locals():
            dataset_name = dataset_name_for_fallback
        else:
            dataset_name = os.path.basename(scenario_path)
        local_output_dir = os.path.join(os.path.expanduser("~"), "checkpoints", f"{dataset_name}_{config_name}")
        output_dir = local_output_dir
        os.makedirs(output_dir, exist_ok=True)
        print(f"ä½¿ç”¨æœ¬åœ°ç›®å½•: {output_dir}")
    
    output_path = os.path.join(output_dir, 'test_leaderboard.json')
    
    # ä¿å­˜ç»“æœï¼ˆåªä¿ç•™test_leaderboard.jsonï¼Œä¿æŒåŸæœ‰ç»“æ„ï¼‰
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"  æ¯ä¸ªæ ·æœ¬ç”Ÿæˆäº† {num_samples} ä¸ª continuations")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  æ³¨æ„: æœ€ç»ˆäº¤ä»˜æ—¶åªéœ€è¦ä¿ç•™ test_leaderboard.json æ–‡ä»¶")


def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨å¾®è°ƒæ¨¡å‹ç”Ÿæˆtest_leaderboard.json')
    parser.add_argument('--checkpoint_dir', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•')
    parser.add_argument('--scenario_path', type=str, required=True,
                       help='åœºæ™¯ç›®å½•è·¯å¾„ï¼ˆåŒ…å«test_leaderboard.jsonï¼‰')
    parser.add_argument('--config_name', type=str, required=True,
                       help='é…ç½®åç§°ï¼ˆç”¨äºè¾“å‡ºç›®å½•å‘½åï¼Œå¦‚ï¼šprofile_and_historyï¼‰')
    parser.add_argument('--use_profile', action='store_true',
                       help='æ˜¯å¦ä½¿ç”¨profile')
    parser.add_argument('--use_history', action='store_true',
                       help='æ˜¯å¦ä½¿ç”¨history')
    parser.add_argument('--use_context', action='store_true', default=True,
                       help='æ˜¯å¦ä½¿ç”¨contextï¼ˆé»˜è®¤ï¼šTrueï¼‰')
    parser.add_argument('--num_samples', type=int, default=5,
                       help='æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„continuationæ•°é‡ï¼ˆé»˜è®¤ï¼š5ï¼‰')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šåœ¨scenario_pathä¸‹åˆ›å»ºoutput_{config_name}ï¼‰')
    parser.add_argument('--gpu', type=int, default=1,
                       help='ä½¿ç”¨çš„GPUç¼–å·ï¼ˆé»˜è®¤ï¼š1ï¼‰')
    parser.add_argument('--log_file', type=str, default=None,
                       help='è¯¦ç»†æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼Œè®°å½•æ¯æ¬¡è°ƒç”¨çš„ä¸Šä¸‹æ–‡ã€è¾“å…¥ã€è¾“å‡ºç­‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--base_model_path', type=str, default=None,
                       help='LoRA æ¨¡å‹çš„åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œå°†å°è¯•è‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--max_new_tokens', type=int, default=512,
                       help='æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤ï¼š512ï¼Œç»™æ¨¡å‹è¶³å¤Ÿç©ºé—´å†™å®Œ"åºŸè¯"ï¼Œåç»­é€šè¿‡æ¸…æ´—å‡½æ•°æˆªæ–­ï¼‰')
    parser.add_argument('--max_output_length', type=int, default=512,
                       help='è¾“å‡ºæ–‡æœ¬æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š200ï¼Œç”¨äºåå¤„ç†æˆªæ–­ï¼‰')
    parser.add_argument('--use_multi_gpu', action='store_true',
                       help='ä½¿ç”¨å¤š GPU åˆ†å¸ƒå¼åŠ è½½æ¨¡å‹ï¼ˆé€‚ç”¨äºå¤§æ¨¡å‹ï¼Œä¼šè‡ªåŠ¨åœ¨æ‰€æœ‰å¯ç”¨ GPU ä¸Šåˆ†å¸ƒï¼‰')
    parser.add_argument('--use_deepspeed', action='store_true',
                       help='ä½¿ç”¨ DeepSpeed ZeRO-3 åŠ è½½æ¨¡å‹ï¼ˆé€‚ç”¨äºè¶…å¤§æ¨¡å‹ï¼Œå¯ä»¥è§£å†³ "header too large" é”™è¯¯ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®GPUï¼ˆç›´æ¥ä½¿ç”¨æŒ‡å®šçš„ GPU ç¼–å·ï¼Œä¸ä½¿ç”¨ CUDA_VISIBLE_DEVICESï¼‰
    target_gpu = args.gpu
    print(f"ç›®æ ‡ GPU: {target_gpu}")
    
    # éªŒè¯ GPU æ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        if target_gpu >= torch.cuda.device_count():
            print(f"è­¦å‘Š: GPU {target_gpu} ä¸å­˜åœ¨ï¼Œå¯ç”¨ GPU: {list(range(torch.cuda.device_count()))}")
            print(f"å°†ä½¿ç”¨ GPU 0")
            target_gpu = 0
        print(f"CUDA å¯ç”¨ï¼ŒGPU æ•°é‡: {torch.cuda.device_count()}")
        print(f"å°†ä½¿ç”¨ GPU: {target_gpu}")
    else:
        print("è­¦å‘Š: CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU")
    
    # å¦‚æœæœªæŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª
    if args.log_file is None:
        # ä» scenario_path æå–æ•°æ®é›†åç§°
        dataset_name = os.path.basename(args.scenario_path)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if args.output_dir:
            log_dir = args.output_dir
        else:
            log_dir = os.path.join(os.path.dirname(args.checkpoint_dir), "logs")
        os.makedirs(log_dir, exist_ok=True)
        args.log_file = os.path.join(log_dir, f"inference_{dataset_name}_{args.config_name}_{timestamp}.json")
    
    # å°† GPU ç¼–å·å’ŒåŸºç¡€æ¨¡å‹è·¯å¾„ä¼ é€’ç»™ process_scenario
    process_scenario(
        scenario_path=args.scenario_path,
        checkpoint_dir=args.checkpoint_dir,
        config_name=args.config_name,
        use_profile=args.use_profile,
        use_history=args.use_history,
        use_context=args.use_context,
        num_samples=args.num_samples,
        output_dir=args.output_dir,
        gpu_id=target_gpu,
        log_file_path=args.log_file,
        base_model_path=args.base_model_path,
        max_new_tokens=args.max_new_tokens,
        max_output_length=args.max_output_length,
        use_multi_gpu=args.use_multi_gpu,
        use_deepspeed=args.use_deepspeed
    )


if __name__ == '__main__':
    main()
