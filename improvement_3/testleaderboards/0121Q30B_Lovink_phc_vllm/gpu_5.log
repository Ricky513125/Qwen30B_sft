ÁõÆÊ†á GPU: 0
CUDA ÂèØÁî®ÔºåGPU Êï∞Èáè: 1
Â∞Ü‰ΩøÁî® GPU: 0

Â§ÑÁêÜÂú∫ÊôØ: /mnt/parallel/GIDigitalTwinBench/IdealSelf/LovinkDialogue
Ê®°Âûã: /mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507
ÈÖçÁΩÆ: profile=True, history=True, context=True
ÊµãËØïÊ†∑Êú¨Êï∞: 126
Êï∞ÊçÆÂàÜÁâá 5/8: Â§ÑÁêÜÊ†∑Êú¨ 75 Âà∞ 90 (ÂÖ± 15 ‰∏™Ê†∑Êú¨)
‚úÖ ÊàêÂäüÂ∞Ü 7736 ‰∏™Ê†∑Êú¨‰øùÂ≠òËá≥: /data/lingyu.li/parallel-post-train/ablation/sample_results/extracted_samples.jsonl
ËÆ≠ÁªÉÊ†∑Êú¨Êï∞: 7736
‰ΩøÁî® vLLM Âä†ÈÄüÊé®ÁêÜ...
vLLM Â∞Ü‰ΩøÁî® GPU 0 (Êò†Â∞Ñ‰∏∫ cuda:0)
ÂàùÂßãÂåñ vLLM ÂºïÊìé...
INFO 01-22 15:48:55 [utils.py:263] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 4096, 'disable_log_stats': True, 'model': '/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 01-22 15:48:55 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
INFO 01-22 15:48:55 [model.py:1545] Using max model len 4096
INFO 01-22 15:48:56 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 01-22 15:48:56 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 01-22 15:48:56 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
WARNING 01-22 15:48:56 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[0;36m(EngineCore_DP0 pid=242345)[0;0m INFO 01-22 15:49:09 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507', speculative_config=None, tokenizer='/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=242345)[0;0m INFO 01-22 15:49:11 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://103.248.57.11:26505 backend=nccl
[0;36m(EngineCore_DP0 pid=242345)[0;0m INFO 01-22 15:49:11 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=242345)[0;0m INFO 01-22 15:49:13 [gpu_model_runner.py:3808] Starting to load model /mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507...
[0;36m(EngineCore_DP0 pid=242345)[0;0m INFO 01-22 15:49:39 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [gpu_model_runner.py:3903] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 348.38 MiB is free. Process 242346 has 17.67 GiB memory in use. Process 242349 has 16.51 GiB memory in use. Process 242361 has 17.67 GiB memory in use. Process 242354 has 16.51 GiB memory in use. Process 242348 has 17.67 GiB memory in use. Including non-PyTorch memory, this process has 18.42 GiB memory in use. Process 242347 has 17.26 GiB memory in use. Process 242362 has 17.67 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 21.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     super().__init__(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self._init_executor()
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3904, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     raise e
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3827, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     model = initialize_model(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 659, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.model = Qwen3MoeModel(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                  ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 305, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 412, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 606, in make_layers
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 414, in <lambda>
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     lambda prefix: Qwen3MoeDecoderLayer(vllm_config=vllm_config, prefix=prefix),
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 352, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.mlp = Qwen3MoeSparseMoeBlock(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 163, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.experts = FusedMoE(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]                    ^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 675, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     self.quant_method.create_weights(layer=self, **moe_quant_params)
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py", line 167, in create_weights
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     torch.empty(
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m ERROR 01-22 15:49:40 [core.py:936] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 348.38 MiB is free. Process 242346 has 17.67 GiB memory in use. Process 242349 has 16.51 GiB memory in use. Process 242361 has 17.67 GiB memory in use. Process 242354 has 16.51 GiB memory in use. Process 242348 has 17.67 GiB memory in use. Including non-PyTorch memory, this process has 18.42 GiB memory in use. Process 242347 has 17.26 GiB memory in use. Process 242362 has 17.67 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 21.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0;36m(EngineCore_DP0 pid=242345)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=242345)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 940, in run_engine_core
[0;36m(EngineCore_DP0 pid=242345)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=242345)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=242345)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=242345)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3904, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3827, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=242345)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=242345)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=242345)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=242345)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 659, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.model = Qwen3MoeModel(
[0;36m(EngineCore_DP0 pid=242345)[0;0m                  ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 305, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 412, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=242345)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 606, in make_layers
[0;36m(EngineCore_DP0 pid=242345)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=242345)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 414, in <lambda>
[0;36m(EngineCore_DP0 pid=242345)[0;0m     lambda prefix: Qwen3MoeDecoderLayer(vllm_config=vllm_config, prefix=prefix),
[0;36m(EngineCore_DP0 pid=242345)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 352, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.mlp = Qwen3MoeSparseMoeBlock(
[0;36m(EngineCore_DP0 pid=242345)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 163, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.experts = FusedMoE(
[0;36m(EngineCore_DP0 pid=242345)[0;0m                    ^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 675, in __init__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     self.quant_method.create_weights(layer=self, **moe_quant_params)
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py", line 167, in create_weights
[0;36m(EngineCore_DP0 pid=242345)[0;0m     torch.empty(
[0;36m(EngineCore_DP0 pid=242345)[0;0m   File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
[0;36m(EngineCore_DP0 pid=242345)[0;0m     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=242345)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=242345)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 348.38 MiB is free. Process 242346 has 17.67 GiB memory in use. Process 242349 has 16.51 GiB memory in use. Process 242361 has 17.67 GiB memory in use. Process 242354 has 16.51 GiB memory in use. Process 242348 has 17.67 GiB memory in use. Including non-PyTorch memory, this process has 18.42 GiB memory in use. Process 242347 has 17.26 GiB memory in use. Process 242362 has 17.67 GiB memory in use. Of the allocated memory 17.74 GiB is allocated by PyTorch, and 21.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W122 15:49:41.170579151 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 1963, in <module>
    main()
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 1936, in main
    process_scenario(
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 960, in process_scenario
    vllm_engine = LLM(**vllm_kwargs)
                  ^^^^^^^^^^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 338, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 176, in from_engine_args
    return cls(
           ^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 110, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 94, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 652, in __init__
    super().__init__(
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 921, in launch_core_engines
    wait_for_engine_startup(
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
