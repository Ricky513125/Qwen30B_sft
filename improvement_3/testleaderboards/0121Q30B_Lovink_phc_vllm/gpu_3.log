ç›®æ ‡ GPU: 0
CUDA å¯ç”¨ï¼ŒGPU æ•°é‡: 1
å°†ä½¿ç”¨ GPU: 0

å¤„ç†åœºæ™¯: /mnt/parallel/GIDigitalTwinBench/IdealSelf/LovinkDialogue
æ¨¡å‹: /mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507
é…ç½®: profile=True, history=True, context=True
æµ‹è¯•æ ·æœ¬æ•°: 126
æ•°æ®åˆ†ç‰‡ 3/8: å¤„ç†æ ·æœ¬ 45 åˆ° 60 (å…± 15 ä¸ªæ ·æœ¬)
âœ… æˆåŠŸå°† 7736 ä¸ªæ ·æœ¬ä¿å­˜è‡³: /data/lingyu.li/parallel-post-train/ablation/sample_results/extracted_samples.jsonl
è®­ç»ƒæ ·æœ¬æ•°: 7736
ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†...
ä½¿ç”¨ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES=3
å¯è§GPUæ•°é‡: 1
GPU 0: NVIDIA H200, æ€»å†…å­˜: 139.80 GB, å¯ç”¨å†…å­˜: 139.80 GB
vLLMé…ç½®: gpu_memory_utilization=0.85, max_model_len=1536
åˆå§‹åŒ– vLLM å¼•æ“...
INFO 01-22 16:11:48 [utils.py:263] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 1536, 'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'model': '/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 01-22 16:11:48 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
INFO 01-22 16:11:48 [model.py:1545] Using max model len 1536
INFO 01-22 16:11:48 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 01-22 16:11:48 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 01-22 16:11:48 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
WARNING 01-22 16:11:49 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:11:58 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507', speculative_config=None, tokenizer='/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:11:59 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://103.248.57.11:22133 backend=nccl
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:11:59 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:00 [gpu_model_runner.py:3808] Starting to load model /mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507...
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:00 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:00<00:14,  1.06it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:01<00:13,  1.04it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:02<00:12,  1.08it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:03<00:11,  1.03it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:04<00:11,  1.00s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:05<00:09,  1.02it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:06<00:09,  1.01s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:07<00:08,  1.00s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:08<00:06,  1.01it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:09<00:06,  1.02s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:10<00:05,  1.00s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:12<00:04,  1.03s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  81% Completed | 13/16 [00:14<00:04,  1.45s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  88% Completed | 14/16 [00:15<00:02,  1.30s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards:  94% Completed | 15/16 [00:16<00:01,  1.23s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:16<00:00,  1.02it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:16<00:00,  1.05s/it]
[0;36m(EngineCore_DP0 pid=43826)[0;0m 
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:17 [default_loader.py:291] Loading weights took 16.87 seconds
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:18 [gpu_model_runner.py:3905] Model loading took 56.93 GiB memory and 17.335294 seconds
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:27 [backends.py:644] Using cache directory: /data/lingyu.li/.cache/vllm/torch_compile_cache/226d7ed3e9/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:27 [backends.py:704] Dynamo bytecode transform time: 8.71 s
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:32 [fused_moe.py:1077] Using configuration from /mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H200.json for MoE layer.
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:33 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 16384) from the cache, took 1.139 s
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:33 [monitor.py:34] torch.compile takes 9.85 s in total
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:34 [gpu_worker.py:358] Available KV cache memory: 55.15 GiB
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:35 [kv_cache_utils.py:1305] GPU KV cache size: 602,384 tokens
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:35 [kv_cache_utils.py:1310] Maximum concurrency for 1,536 tokens per request: 392.18x
[0;36m(EngineCore_DP0 pid=43826)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/51 [00:00<00:05,  8.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:05,  8.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/51 [00:00<00:05,  8.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–‰         | 5/51 [00:00<00:04,  9.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:04,  9.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:04,  9.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/51 [00:00<00:04,  9.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:01<00:04,  9.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:01<00:04,  8.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 12/51 [00:01<00:04,  9.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:01<00:04,  9.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:01<00:03,  9.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:01<00:03,  9.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:01<00:03,  9.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–      | 17/51 [00:01<00:03,  9.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:01<00:03,  9.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:02<00:04,  7.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:02<00:04,  7.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:02<00:03,  7.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/51 [00:02<00:03,  8.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:02<00:03,  8.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:02<00:03,  8.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:02<00:02,  8.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:02<00:02,  9.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/51 [00:03<00:02,  8.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:03<00:02,  8.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:03<00:02,  8.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:03<00:02,  8.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:03<00:02,  8.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/51 [00:03<00:02,  9.11it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:03<00:01,  9.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:03<00:01,  9.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:03<00:01,  9.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:04<00:01,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/51 [00:04<00:01,  8.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:04<00:01,  8.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 39/51 [00:04<00:01,  8.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:04<00:01,  8.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 41/51 [00:04<00:01,  8.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:04<00:01,  8.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 43/51 [00:04<00:00,  8.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:04<00:00,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 45/51 [00:05<00:00,  9.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:05<00:00,  9.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/51 [00:05<00:00,  8.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:05<00:00,  8.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 49/51 [00:05<00:00,  8.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:05<00:00,  8.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:05<00:00,  8.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:05<00:00,  8.83it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   4%|â–         | 2/51 [00:00<00:05,  9.54it/s]Capturing CUDA graphs (decode, FULL):   8%|â–Š         | 4/51 [00:00<00:04, 10.26it/s]Capturing CUDA graphs (decode, FULL):  12%|â–ˆâ–        | 6/51 [00:00<00:04, 10.59it/s]Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:04, 10.55it/s]Capturing CUDA graphs (decode, FULL):  20%|â–ˆâ–‰        | 10/51 [00:00<00:03, 10.77it/s]Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–       | 12/51 [00:01<00:04,  8.44it/s]Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 13/51 [00:01<00:04,  8.52it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–‰       | 15/51 [00:01<00:03,  9.32it/s]Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–      | 17/51 [00:01<00:03,  9.77it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [00:01<00:03, 10.10it/s]Capturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [00:02<00:02, 10.42it/s]Capturing CUDA graphs (decode, FULL):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:02<00:02, 10.46it/s]Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [00:02<00:02, 10.64it/s]Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/51 [00:02<00:02, 10.88it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:02<00:01, 11.02it/s]Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [00:03<00:01, 11.23it/s]Capturing CUDA graphs (decode, FULL):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:03<00:01, 10.93it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 35/51 [00:03<00:01, 10.81it/s]Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/51 [00:03<00:01,  9.67it/s]Capturing CUDA graphs (decode, FULL):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:03<00:01,  9.64it/s]Capturing CUDA graphs (decode, FULL):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:03<00:01, 10.24it/s]Capturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:04<00:00, 10.74it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:04<00:00, 11.09it/s]Capturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:04<00:00, 11.31it/s]Capturing CUDA graphs (decode, FULL):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:04<00:00, 11.32it/s]Capturing CUDA graphs (decode, FULL):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:04<00:00, 11.17it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:04<00:00, 10.45it/s]
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:46 [gpu_model_runner.py:4856] Graph capturing finished in 11 secs, took 1.48 GiB
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:46 [core.py:273] init engine (profile, create kv cache, warmup model) took 28.58 seconds
[0;36m(EngineCore_DP0 pid=43826)[0;0m INFO 01-22 16:12:47 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 01-22 16:12:47 [llm.py:347] Supported tasks: ['generate']
`torch_dtype` is deprecated! Use `dtype` instead!
âœ“ vLLM å¼•æ“åŠ è½½æˆåŠŸ
  ä½¿ç”¨å• GPU æ¨¡å¼ï¼Œç›®æ ‡è®¾å¤‡: cuda:0
  ä½¿ç”¨æ•°æ®ç±»å‹: bfloat16
  å›é€€åˆ°æŒ‡å®šè®¾å¤‡åŠ è½½: cuda:0
Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:   6%|â–‹         | 1/16 [00:01<00:17,  1.17s/it]Loading checkpoint shards:  12%|â–ˆâ–        | 2/16 [00:02<00:16,  1.15s/it]Loading checkpoint shards:  19%|â–ˆâ–‰        | 3/16 [00:03<00:14,  1.13s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:04<00:12,  1.07s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:05<00:16,  1.33s/it]
  æŒ‡å®šè®¾å¤‡åŠ è½½ä¹Ÿå¤±è´¥: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 7.81 MiB is free. Including non-PyTorch memory, this process has 18.75 GiB memory in use. Process 43826 has 121.03 GiB memory in use. Of the allocated memory 18.14 GiB is allocated by PyTorch, and 114.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  ä½¿ç”¨ device_map åŠ è½½å¤±è´¥: å• GPU åŠ è½½å¤±è´¥: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 7.81 MiB is free. Including non-PyTorch memory, this process has 18.75 GiB memory in use. Process 43826 has 121.03 GiB memory in use. Of the allocated memory 18.14 GiB is allocated by PyTorch, and 114.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
è¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼
  å°è¯•ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼åŠ è½½...
  å°è¯•æœ€ç®€å•çš„åŠ è½½æ–¹å¼...
Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–        | 2/16 [00:00<00:01, 13.02it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:00<00:01, 11.93it/s]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:00<00:00, 11.57it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 11.81it/s]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 10/16 [00:00<00:00, 11.96it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:01<00:00, 12.01it/s]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:01<00:00, 11.85it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:01<00:00, 13.29it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:01<00:00, 12.43it/s]
  ç®€å•æ–¹å¼åŠ è½½ä¹Ÿå¤±è´¥: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 5.81 MiB is free. Including non-PyTorch memory, this process has 18.75 GiB memory in use. Process 43826 has 121.03 GiB memory in use. Of the allocated memory 18.19 GiB is allocated by PyTorch, and 57.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 1452, in process_scenario
    raise RuntimeError(f"å• GPU åŠ è½½å¤±è´¥: {last_error}\nè¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼")
RuntimeError: å• GPU åŠ è½½å¤±è´¥: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 7.81 MiB is free. Including non-PyTorch memory, this process has 18.75 GiB memory in use. Process 43826 has 121.03 GiB memory in use. Of the allocated memory 18.14 GiB is allocated by PyTorch, and 114.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
è¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 1566, in process_scenario
    model = model.to(target_device)
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4343, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/mnt/parallel/Qwen_30B_try/lingyu/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 5.81 MiB is free. Including non-PyTorch memory, this process has 18.75 GiB memory in use. Process 43826 has 121.03 GiB memory in use. Of the allocated memory 18.19 GiB is allocated by PyTorch, and 57.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 1572, in process_scenario
    raise RuntimeError(f"æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}\nè¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼")
RuntimeError: æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ã€‚æœ€åé”™è¯¯: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 5.81 MiB is free. Including non-PyTorch memory, this process has 18.75 GiB memory in use. Process 43826 has 121.03 GiB memory in use. Of the allocated memory 18.19 GiB is allocated by PyTorch, and 57.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
è¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 2028, in <module>
    main()
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 2001, in main
    process_scenario(
  File "/mnt/parallel/Qwen_30B_try/improvement_3/inference.py", line 1591, in process_scenario
    raise RuntimeError(error_msg)
RuntimeError: 
æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ã€‚é”™è¯¯: æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ã€‚æœ€åé”™è¯¯: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 139.80 GiB of which 5.81 MiB is free. Including non-PyTorch memory, this process has 18.75 GiB memory in use. Process 43826 has 121.03 GiB memory in use. Of the allocated memory 18.19 GiB is allocated by PyTorch, and 57.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
è¯·å°è¯•è½¬æ¢æ¨¡å‹ä¸º PyTorch æ ¼å¼

æ¨¡å‹æ–‡ä»¶æ£€æŸ¥ç»“æœ:
  - Safetensors æ–‡ä»¶: 0 ä¸ª
  - PyTorch æ–‡ä»¶: 0 ä¸ª

å»ºè®®çš„è§£å†³æ–¹æ¡ˆï¼š
1. å°† safetensors è½¬æ¢ä¸º PyTorch æ ¼å¼:
   python -c "from transformers import AutoModel; model = AutoModel.from_pretrained('/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507', trust_remote_code=True); model.save_pretrained('/mnt/parallel/models/Qwen3-30B-A3B-Instruct-2507_pytorch', safe_serialization=False)"

2. æ›´æ–°åº“ç‰ˆæœ¬:
   pip install --upgrade safetensors transformers accelerate

3. ä½¿ç”¨å• GPU æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ --use_multi_gpuï¼‰

[rank0]:[W122 16:13:05.521855407 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR 01-22 16:13:05 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
